{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 05a\n",
    "\n",
    "Run different models on ctec dataset itself without jigsaw dataset. \n",
    "\n",
    "Given that ctec dataset is imbalanced and has overwhelmingly more positive examples, we upsample negative examples. \n",
    "\n",
    "<ul>\n",
    "    <li>Tokenizer: BERT</li>\n",
    "    <li>Training set: 10800 ctec positive + 1350 ctec negative (8:1 ratio) </li>\n",
    "    <li>Validation set: 1200 ctec positive + 150 ctec negative (8:1 ratio) </li>\n",
    "    <li>Test set: 9600 ctec positive + 401 ctec negative</li>\n",
    "    <li>Upsampling technique: <span style=\"color:red\"><b>naive duplication</b></span></li>\n",
    "    <li>Models: Logistic Regression, Multilayer Perceptron, BERT</li>\n",
    "</ul>\n",
    "\n",
    "## Part 1: Preparation\n",
    "\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 0 --> testing mode \n",
    "# 1 --> development mode \n",
    "toy_mode = 0\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "from math import inf\n",
    "\n",
    "# Define constants \n",
    "MAX_LEN = 300    # Max text length for encoding purpose \n",
    "BATCH_SIZE = 16\n",
    "PRETRAINED_BERT_NAME = 'bert-base-uncased'\n",
    "SESS_PWD = './xprmt_05a/'\n",
    "\n",
    "# Enable GPU if possible \n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctec # negative examples = 1901\n",
      "ctec # positive examples = 21600\n"
     ]
    }
   ],
   "source": [
    "# Load ctec dataset \n",
    "ctec_df = pd.read_csv('ctec_training_data_preproc.csv')\n",
    "\n",
    "ctec_neg = ctec_df[ctec_df['label'] == 0]\n",
    "ctec_pos = ctec_df[ctec_df['label'] == 1]\n",
    "\n",
    "print(f'ctec # negative examples = {ctec_neg.shape[0]}')\n",
    "print(f'ctec # positive examples = {ctec_pos.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sampling indices \n",
    "np.random.seed(42)\n",
    "\n",
    "# Indices for train & validation set \n",
    "indpos_trainval = np.random.choice(range(ctec_pos.shape[0]), size = 12000, replace = False)\n",
    "indneg_trainval = np.random.choice(range(ctec_neg.shape[0]), size = 1500, replace = False)\n",
    "\n",
    "# Indices for test set \n",
    "indpos_test = np.setdiff1d(range(ctec_pos.shape[0]), indpos_trainval)\n",
    "indneg_test = np.setdiff1d(range(ctec_neg.shape[0]), indneg_trainval)\n",
    "\n",
    "# Indices for train set \n",
    "indpos_train = np.random.choice(indpos_trainval, size = 10800, replace = False)\n",
    "indneg_train = np.random.choice(indneg_trainval, size = 1350, replace = False)\n",
    "\n",
    "# Indices for cross-validation set \n",
    "indpos_val = np.setdiff1d(indpos_trainval, indpos_train)\n",
    "indneg_val = np.setdiff1d(indneg_trainval, indneg_train)\n",
    "\n",
    "ctec_train_origin = ctec_pos.iloc[indpos_train].append(\n",
    "    ctec_neg.iloc[indneg_train], \n",
    "    ignore_index = True\n",
    ")\n",
    "\n",
    "ctec_val = ctec_pos.iloc[indpos_val].append(\n",
    "    ctec_neg.iloc[indneg_val], \n",
    "    ignore_index = True\n",
    ")\n",
    "\n",
    "ctec_test = ctec_pos.iloc[indpos_test].append(\n",
    "    ctec_neg.iloc[indneg_test], \n",
    "    ignore_index = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample training set by naively duplicating negative examples \n",
    "# pos:neg = 8:3 ratio \n",
    "ctec_train_upsamp = ctec_train_origin.append(\n",
    "    ctec_neg.iloc[indneg_train], \n",
    "    ignore_index = True\n",
    ").append(\n",
    "    ctec_neg.iloc[indneg_train], \n",
    "    ignore_index = True\n",
    ")\n",
    "\n",
    "# Shuffle the upsampled data\n",
    "ctec_train_unsamp = ctec_train_upsamp.sample(n = ctec_train_upsamp.shape[0], replace = False).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input and label\n",
    "\n",
    "`toy_mode = 1` means we are in development mode and will only load very little number of dataset, so that we can debug without wasting time wating for results. \n",
    "\n",
    "`toy_mode = 0` means we are in testing mode and will load all data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = ctec_train_upsamp['comment_text']\n",
    "y_train = ctec_train_upsamp['label']\n",
    "X_val_text = ctec_val['comment_text']\n",
    "y_val = ctec_val['label']\n",
    "X_test_text = ctec_test['comment_text']\n",
    "y_test = ctec_test['label']\n",
    "\n",
    "if toy_mode: \n",
    "    X_train_text = X_train_text[:50]\n",
    "    y_train = y_train[:50]\n",
    "    X_val_text = X_val_text[:30]\n",
    "    y_val = y_val[:30]\n",
    "    X_test_text = X_test_text[:20]\n",
    "    y_test = y_test[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience of creating PyTorch `DataSet`, we will will fit `CountVectorizer()` and `TfidfVectorizer` for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVect = CountVectorizer()\n",
    "countVect.fit(X_train_text)\n",
    "\n",
    "tfidfVect = TfidfVectorizer()\n",
    "tfidfVect.fit(X_train_text)\n",
    "\n",
    "bertTokenizer = BertTokenizer.from_pretrained(PRETRAINED_BERT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text encoding \\& creating PyTorch `Dataset`, `DataLoader`\n",
    "\n",
    "To utilize PyTorch and GPU computation, we create instances of `Dataset` instead of using our original dataset. \n",
    "\n",
    "<b>Notice. </b> Text encoding is performed inside `__getitem()__` function of `Dataset` class. \n",
    "\n",
    "`DataLoader` provides a way to iterate through a dataset with given batch size. \n",
    "\n",
    "<b style=\"color:red\">Attention! </b> All first-initialized PyTorch tensors must be moved to GPU memory manually! Pay attention when you initialize a PyTorch tensor! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicDataset(Dataset): \n",
    "    def __init__(self, texts, labels, tokenizer, max_len = MAX_LEN): \n",
    "        '''\n",
    "        Instantiate ToxicDataset() class \n",
    "        @Params\n",
    "        -- texts: comments texts of the dataset (input)\n",
    "        -- labels: labels corresponding to the texts \n",
    "        -- tokenizer: {'bow', 'tfidf', 'bert'} the scheme of encoding texts into numerical data\n",
    "        -- max_len: if we use BERT tokenizer, we represent texts with `max_len`-dim vectors\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.texts = texts \n",
    "        self.labels = labels \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len \n",
    "        \n",
    "    def __len__(self): \n",
    "        '''Return the size of dataset '''\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        '''This method must be overwritten by programmer '''\n",
    "        \n",
    "        text = str(self.texts[idx])\n",
    "        # Vector embedding of text. Will be computed by tokenizer \n",
    "        input_vec = None\n",
    "        # Must be PyTorch tensor \n",
    "        label = torch.tensor(\n",
    "            self.labels[idx], \n",
    "            dtype = torch.long\n",
    "        ).to(device)\n",
    "        # Only applicable to BERT\n",
    "        attention_mask = None\n",
    "        \n",
    "        '''\n",
    "        Encode text data according to the given tokenizer\n",
    "        The result of encoding must be PyTorch Tensor. \n",
    "        '''\n",
    "        # BoW encoding \n",
    "        if self.tokenizer == 'bow': \n",
    "            input_vec = torch.Tensor(\n",
    "                countVect.transform([text]).toarray()\n",
    "            ).to(device).flatten()\n",
    "            \n",
    "        # TF-IDF encoding\n",
    "        elif self.tokenizer == 'tfidf': \n",
    "            input_vec = torch.Tensor(\n",
    "                tfidfVect.transform([text]).toarray()\n",
    "            ).to(device).flatten()\n",
    "            \n",
    "        # BERT encoding \n",
    "        elif self.tokenizer == 'bert': \n",
    "            encoding = bertTokenizer.encode_plus(\n",
    "                text, \n",
    "                add_special_tokens = True, \n",
    "                truncation = True, \n",
    "                max_length = self.max_len, \n",
    "                return_token_type_ids = False, \n",
    "                pad_to_max_length = True, \n",
    "                return_attention_mask = True, \n",
    "                return_tensors = 'pt'\n",
    "            )\n",
    "            \n",
    "            input_vec = encoding['input_ids'].to(device).flatten()\n",
    "            attention_mask = encoding['attention_mask'].to(device).flatten()\n",
    "            \n",
    "        return {\n",
    "            'text': text, \n",
    "            'input_vec': input_vec, \n",
    "            'input_size': input_vec.shape[0], \n",
    "            'attention_mask': attention_mask, \n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(\n",
    "    texts, labels, \n",
    "    tokenizer, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    max_len = MAX_LEN\n",
    "): \n",
    "    '''\n",
    "    Helper function for creating DataLoader.  \n",
    "    @Params\n",
    "    -- texts: comments texts of the dataset (input)\n",
    "    -- labels: labels corresponding to the texts \n",
    "    -- tokenizer: {'bow', 'tfidf', 'bert'} the scheme of encoding texts into numerical data\n",
    "    -- batch_size: (as the name suggests)\n",
    "    -- max_len: if we use BERT tokenizer, we represent texts with `max_len`-dim vectors\n",
    "    '''\n",
    "    \n",
    "    dataset = ToxicDataset(\n",
    "        texts = texts, \n",
    "        labels = labels, \n",
    "        tokenizer = tokenizer, \n",
    "        max_len = max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size, \n",
    "        num_workers = 0    # We can change this value to enable multiprocessing\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally instantiate `DataLoader`s for training, validation, and test set. \n",
    "\n",
    "(For now) we only use BERT tokenizer and leave alone BoW and TF-IDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = create_data_loader(\n",
    "    X_train_text, y_train, 'bert'\n",
    ")\n",
    "\n",
    "val_data_loader = create_data_loader(\n",
    "    X_val_text, y_val, 'bert'\n",
    ")\n",
    "\n",
    "test_data_loader = create_data_loader(\n",
    "    X_test_text, y_test, 'bert'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>A key question I have not solved</b>. Normalizing features can usually boost the performance of machine learning models. Under the current coding style where text encoding is performed inside `Dataset`, how do we write the function for normalizing features? \n",
    "\n",
    "Pros and cons for writing the encoding routine inside `Dataset`: \n",
    "\n",
    "<ul>\n",
    "    <li>Pros: can specify encoding scheme by an argument, and will be convenience for grid-searching. </li>\n",
    "    <li>Cons: hard to implement normalization and other possible routines. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Models and model-specific parameters\n",
    "\n",
    "### Create model class\n",
    "\n",
    "We define our models / classifiers by inherting and overwriting `nn.Module` class. \n",
    "\n",
    "<b>Model-specific hyperparameters</b> (<span style=\"color:blue;\">hidden layers, drop-out rate, etc.<span/>) should be defined as attributes of each model class. \n",
    "    \n",
    "<b style=\"color:red\">Attention! </b> `__init__()` must use clases in `torch.nn` module specify inter-layer operations that (1) have parameters to train or (2) change dimensionality (e.g. linear map, convolution, dropout); on the other hand, all operations within layer without trained parameters or change in dimensionality can either be treated as separate layter using `torch.nn` or as the same layer using `torch.nn.Functional`. \n",
    "\n",
    "#### BERT classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):     \n",
    "    def __init__(self, num_labels, drop_out = 0.2):   \n",
    "        '''\n",
    "        @Params\n",
    "        -- drop_out: Drop out rate. By default = 0.2\n",
    "        '''\n",
    "        \n",
    "        # Must instantiate the parent class \n",
    "        super(BertClassifier, self).__init__()\n",
    "        \n",
    "        # pretrained BERT model\n",
    "        self.bert = BertModel.from_pretrained(PRETRAINED_BERT_NAME)\n",
    "        \n",
    "        # Dropout rate \n",
    "        self.drop = nn.Dropout(p = drop_out)\n",
    "        \n",
    "        # Operation from BERT's last hidden layer to output layer\n",
    "        # A linear map to a 2-dimensional vector (2 === binary classification)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_vec, **kwargs): \n",
    "        '''\n",
    "        Define the feed-forward function of the model\n",
    "        @kwargs\n",
    "        -- attention_mask: the attention mask for BERT\n",
    "        '''\n",
    "        # BERT: from input layer to the last hidden layer \n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids = input_vec, \n",
    "            attention_mask = kwargs['attention_mask']\n",
    "        )\n",
    "        \n",
    "        # Drop out neurons according to dropout rate \n",
    "        output = self.drop(pooled_output)\n",
    "        \n",
    "        # Return the last layer \n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer perceptron classifier\n",
    "\n",
    "i.e. the naive deep neural network\n",
    "\n",
    "<b style=\"color:red;\">Attention!</b> To duplicate a PyTorch tensor, we must use `.clone()` function so that the original tensor will not be accidentally overwritten. \n",
    "\n",
    "<b>Choice of coding style</b>. MLP can be written with either `nn.Module` or `nn.Sequential`. For now we choose the former because `nn.Sequential` does not have well-defined methods (such as `.add()` in TensorFlow). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module): \n",
    "    def __init__(self, input_size, num_labels, hidden_layers_dim, drop_out = 0.2): \n",
    "        '''\n",
    "        @Params\n",
    "        -- input_size: the dimensionality of input vector \n",
    "        -- num_labels: the number of classes \n",
    "        -- hidden_layers_dim: the list of dimensionalities for each hidden layer\n",
    "        '''\n",
    "        \n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        curr_layer_dim = input_size\n",
    "        self.hidden_layers = nn.ModuleList([])\n",
    "        \n",
    "        for dim in hidden_layers_dim: \n",
    "            self.hidden_layers.append(nn.Linear(curr_layer_dim, dim))\n",
    "            self.hidden_layers.append(nn.ReLU())\n",
    "            curr_layer_dim = dim\n",
    "            \n",
    "        self.exit_layer = nn.Linear(curr_layer_dim, num_labels)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_vec, **kwargs): \n",
    "        '''\n",
    "        Define the feed forward function\n",
    "        '''\n",
    "        vec = input_vec.clone()\n",
    "        \n",
    "        # Hidden layers\n",
    "        for layer in self.hidden_layers: \n",
    "            vec = layer(vec)\n",
    "            \n",
    "        # Exit layer\n",
    "        return F.softmax(self.exit_layer(vec), dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression \n",
    "\n",
    "Logistic regression is nothing but MLP with no hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier(MLPClassifier): \n",
    "    def __init__(self, input_size, num_labels = 2): \n",
    "        super().__init__(\n",
    "            input_size, \n",
    "            num_labels, \n",
    "            hidden_layers_dim = []\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Helper functions for training an epoch and evaluating model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Choice of coding style</b>. Unlike TensorFlow and scikit-learn, most PyTorch code I have seen defines the training loops outside their corresponding model classes. This coding style yields better flexibility and controlling power while sacrificing compactness and reusability of code. In other words, it is not common to call something like `model.fit()` in PyTorch. \n",
    "\n",
    "I am not sure whether writing the training functions inside model classes will cause problems, because the optimizer depends on `model.parameters()`. \n",
    "\n",
    "For now, we will write training functions outside model classes. \n",
    "\n",
    "Consequently, we tune the <b>non-model-specific parameters</b> (<span style=\"color:blue;\">optimizer, loss function, number of iteration, number of epoch, etc.</span>) outside model classes. However, these parameters can sometimes be model specific and we need to make ad hoc adjustments to our code. \n",
    "\n",
    "<b style=\"color:red;\">Warning!</b> Code for computing metrics is only applicable to binary classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_binary(\n",
    "    model, data_loader, \n",
    "    loss_fn, optimizer, \n",
    "    device, \n",
    "    scheduler = None, \n",
    "    clip_grad = False  # Enable gradient clipping? \n",
    "): \n",
    "    '''\n",
    "    The helper function that trains one epoch \n",
    "    !! Code for computing metrics is only applicable to binary classification\n",
    "    '''\n",
    "    # Set the model to training mode \n",
    "    model = model.train()\n",
    "    # Clean GPU cache \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Record loss of training on each batch \n",
    "    losses = []\n",
    "    # label, pred_class \n",
    "    # index 00 --> true negative \n",
    "    # index 01 --> false postive \n",
    "    # index 10 --> false negative \n",
    "    # index 11 --> true positive \n",
    "    cat_count = [0, 0, 0, 0]\n",
    "    batch_counter = 0\n",
    "    \n",
    "    # Train each batch \n",
    "    for batch in data_loader: \n",
    "        print(f'\\rTraining batch #{batch_counter} out of {len(data_loader)}', end = '')\n",
    "        \n",
    "        # Load data from current batch \n",
    "        input_vec = batch['input_vec'].to(device)\n",
    "        if not isinstance(model, BertClassifier): \n",
    "            # different models require different datatypes \n",
    "            input_vec = input_vec.float()\n",
    "        label = batch['label'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        # Forward pass \n",
    "        output = None\n",
    "        if isinstance(model, BertClassifier): \n",
    "            '''Need to pass attention mask if model is BERT'''\n",
    "            output = model.forward(input_vec = input_vec, attention_mask = attention_mask)\n",
    "        else: \n",
    "            '''Otherwise, only input_vec is required argument'''\n",
    "            output = model.forward(input_vec = input_vec)\n",
    "            \n",
    "        # Probability of predicted class and predicted class \n",
    "        # torch.max with dim==1 returns (maxval, argmax)\n",
    "        pred_prob, pred_class = torch.max(output, dim = 1)\n",
    "            \n",
    "        # Compute loss \n",
    "        loss = loss_fn(output, label)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Backprop \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping \n",
    "        if clip_grad: \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.)\n",
    "            \n",
    "        # Update optimizer and scheduler \n",
    "        # scheduler is only used for BERT\n",
    "        optimizer.step()\n",
    "        if scheduler: \n",
    "            scheduler.step()\n",
    "        \n",
    "        # Compute which category each examples falls in\n",
    "        # 00 --> true negative \n",
    "        # 01 --> false positive \n",
    "        # 10 --> false negative \n",
    "        # 11 --> true positive \n",
    "        cats = 2 * label  + pred_class\n",
    "        # Count each category \n",
    "        for i in range(4): \n",
    "            cat_count[i] += (cats == i).sum().item()\n",
    "            \n",
    "        # Post-processing \n",
    "        # Failure to do so (especially clearing optimizer) will result in unexpected problems \n",
    "        loss.detach() # delete computational history \n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        batch_counter += 1\n",
    "            \n",
    "    '''Metrics of epoch'''\n",
    "    # Train accuracy of epoch\n",
    "    acc = (cat_count[0] + cat_count[3]) / np.sum(cat_count)\n",
    "    # Confusion matrix \n",
    "    confusion = np.array([[cat_count[0], cat_count[1]], [cat_count[2], cat_count[3]]])\n",
    "    # F1 score assuming positive example is scarce\n",
    "    try:\n",
    "        f1_pos = cat_count[3] / (cat_count[3] + .5 * cat_count[1] + .5 * cat_count[2])\n",
    "    except ZeroDivisionError: \n",
    "        f1_pos = inf\n",
    "    # F1 score assuming negative example is scarce \n",
    "    try:\n",
    "        f1_neg = cat_count[0] / (cat_count[0] + .5 * cat_count[1] + .5 * cat_count[2])\n",
    "    except ZeroDivisionError: \n",
    "        f1_neg = inf\n",
    "    \n",
    "    print()\n",
    "    return np.mean(losses), confusion, acc, f1_pos, f1_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch_binary(\n",
    "    model, data_loader, \n",
    "    loss_fn, optimizer, \n",
    "    device, \n",
    "    scheduler = None, \n",
    "    test_mode = False\n",
    "): \n",
    "    '''\n",
    "    The helper function that evaluates the model\n",
    "    Runs only forward pass without backprop\n",
    "    Primarily used for cross-validation\n",
    "    !! Code for computing metrics is only applicable to binary classification\n",
    "    '''\n",
    "    \n",
    "    # Set the model to training mode \n",
    "    model = model.eval()\n",
    "    # Clean GPU cache \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Record loss of training on each batch \n",
    "    losses = []\n",
    "    # label, pred_class \n",
    "    # index 00 --> true negative \n",
    "    # index 01 --> false postive \n",
    "    # index 10 --> false negative \n",
    "    # index 11 --> true positive \n",
    "    cat_count = [0, 0, 0, 0]\n",
    "    batch_counter = 0\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for batch in data_loader: \n",
    "            if not test_mode: \n",
    "                print(f'\\rCross-validating batch #{batch_counter} out of {len(data_loader)}', end = '')\n",
    "\n",
    "            # Load data from current batch\n",
    "            input_vec = batch['input_vec'].to(device)\n",
    "            if not isinstance(model, BertClassifier): \n",
    "                # different models require different datatypes \n",
    "                input_vec = input_vec.float()\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass \n",
    "            output = None\n",
    "            if isinstance(model, BertClassifier): \n",
    "                '''Need to pass attention mask if model is BERT'''\n",
    "                output = model.forward(input_vec = input_vec, attention_mask = attention_mask)\n",
    "            else: \n",
    "                '''Otherwise, only input_vec is required argument'''\n",
    "                output = model.forward(input_vec = input_vec)\n",
    "\n",
    "            # torch.max with dim=1 returns (maxvals, indices)\n",
    "            # indices are the labels we want to predict \n",
    "            preds_prob, preds_class = torch.max(output, dim = 1)\n",
    "\n",
    "            # Compute which category each examples falls in\n",
    "            # 00 --> true negative \n",
    "            # 01 --> false positive \n",
    "            # 10 --> false negative \n",
    "            # 11 --> true positive \n",
    "            cats = 2 * label  + preds_class\n",
    "            # Count each category \n",
    "            for i in range(4): \n",
    "                cat_count[i] += (cats == i).sum().item()\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(output, label)\n",
    "\n",
    "            # For analysis purpose \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            batch_counter += 1\n",
    "            \n",
    "    # Train accuracy of epoch\n",
    "    acc = (cat_count[0] + cat_count[3]) / np.sum(cat_count)\n",
    "    # Confusion matrix \n",
    "    confusion = np.array([[cat_count[0], cat_count[1]], [cat_count[2], cat_count[3]]])\n",
    "    # F1 score assuming positive example is scarce\n",
    "    try:\n",
    "        f1_pos = cat_count[3] / (cat_count[3] + .5 * cat_count[1] + .5 * cat_count[2])\n",
    "    except ZeroDivisionError: \n",
    "        f1_pos = inf\n",
    "    # F1 score assuming negative example is scarce \n",
    "    try:\n",
    "        f1_neg = cat_count[0] / (cat_count[0] + .5 * cat_count[1] + .5 * cat_count[2])\n",
    "    except ZeroDivisionError: \n",
    "        f1_neg = inf\n",
    "    print()\n",
    "\n",
    "    if test_mode: \n",
    "        print(f'# True negative = {cat_count[0]}')\n",
    "        print(f'# False positive = {cat_count[1]}')\n",
    "        print(f'# False negative = {cat_count[2]}')\n",
    "        print(f'# True positive = {cat_count[3]}')\n",
    "        \n",
    "    return np.mean(losses), confusion, acc, f1_pos, f1_neg \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a metric table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = ['model_name', 'test_confusion_matrix', 'test_accuracy', 'test_f1_pos', 'test_f1_neg']\n",
    "\n",
    "metric_table = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "<b style=\"color:red;\">Attention!</b> Instances of <span style=\"color:blue;\">model classes</span> and <span style=\"color:blue;\">loss functions</span> should also be moved to GPU! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 1 out of 5\n",
      "----------\n",
      "Training batch #928 out of 929\n",
      "Train loss = 0.6065847526261317, Train accuracy = 0.7072727272727273, Train f1_pos = 0.8274794618406953, Train f1_neg = 0.034643570952698204\n",
      "Cross-validating batch #84 out of 85\n",
      "Validation loss = 0.4404677713618559, Validation accuracy = 0.8792592592592593, Validation f1_pos = 0.9356494275562574, Validation f1_neg = 0.023952095808383235\n",
      "----------\n",
      "End epoch 1 out of 5\n",
      "\n",
      "Start epoch 2 out of 5\n",
      "----------\n",
      "Training batch #928 out of 929\n",
      "Train loss = 0.5916531764139272, Train accuracy = 0.7222895622895623, Train f1_pos = 0.8381094449242364, Train f1_neg = 0.024136299100804542\n",
      "Cross-validating batch #84 out of 85\n",
      "Validation loss = 0.4360559708931867, Validation accuracy = 0.8837037037037037, Validation f1_pos = 0.9382618953991348, Validation f1_neg = 0.0\n",
      "----------\n",
      "End epoch 2 out of 5\n",
      "\n",
      "Start epoch 3 out of 5\n",
      "----------\n",
      "Training batch #928 out of 929\n",
      "Train loss = 0.5892480442739534, Train accuracy = 0.7246464646464646, Train f1_pos = 0.8400422485623753, Train f1_neg = 0.011602610587382161\n",
      "Cross-validating batch #84 out of 85\n",
      "Validation loss = 0.4353205182973076, Validation accuracy = 0.8844444444444445, Validation f1_pos = 0.9386792452830188, Validation f1_neg = 0.0\n",
      "----------\n",
      "End epoch 3 out of 5\n",
      "\n",
      "Start epoch 4 out of 5\n",
      "----------\n",
      "Training batch #928 out of 929\n",
      "Train loss = 0.5888989373031037, Train accuracy = 0.7250505050505051, Train f1_pos = 0.8403144432711487, Train f1_neg = 0.011619462599854757\n",
      "Cross-validating batch #84 out of 85\n",
      "Validation loss = 0.4353204730679007, Validation accuracy = 0.8844444444444445, Validation f1_pos = 0.9386792452830188, Validation f1_neg = 0.0\n",
      "----------\n",
      "End epoch 4 out of 5\n",
      "\n",
      "Start epoch 5 out of 5\n",
      "----------\n",
      "Training batch #928 out of 929\n",
      "Train loss = 0.5888963139288136, Train accuracy = 0.7250505050505051, Train f1_pos = 0.8403144432711487, Train f1_neg = 0.011619462599854757\n",
      "Cross-validating batch #84 out of 85\n",
      "Validation loss = 0.43532046290004955, Validation accuracy = 0.8844444444444445, Validation f1_pos = 0.9386792452830188, Validation f1_neg = 0.0\n",
      "----------\n",
      "End epoch 5 out of 5\n",
      "\n",
      "\n",
      "# True negative = 2\n",
      "# False positive = 399\n",
      "# False negative = 37\n",
      "# True positive = 9563\n",
      "Test loss = 0.3582902938698808, Test accuracy = 0.9564043595640436, Test f1_pos = 0.9777118903997546, Test f1_neg = 0.00909090909090909\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 872 ms, total: 1min 26s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''Step 1: set-up'''\n",
    "\n",
    "# This is a pathetic way to retrieve the dimensionality of input vector\n",
    "input_size = next(iter(train_data_loader))['input_size'][0].item()\n",
    "\n",
    "model = LogisticRegressionClassifier(input_size = input_size).to(device)\n",
    "\n",
    "model_name = 'logistic_regression'\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 2e-4)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "num_epoch = 5\n",
    "\n",
    "best_acc = 0\n",
    "best_f1_pos = 0\n",
    "best_f1_neg = 0\n",
    "\n",
    "'''Step 2: training loop '''\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epoch): \n",
    "    print(f'Start epoch {epoch + 1} out of {num_epoch}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_confusion, train_acc, train_f1_pos, train_f1_neg = train_epoch_binary(\n",
    "        model, train_data_loader, \n",
    "        loss_fn, optimizer, \n",
    "        device, \n",
    "        clip_grad = False  \n",
    "    )\n",
    "    \n",
    "    # Record loss to plot learning curve \n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(f'Train loss = {np.mean(train_loss)}', end = ', ')\n",
    "    print(f'Train accuracy = {train_acc}', end = ', ')\n",
    "    print(f'Train f1_pos = {train_f1_pos}', end = ', ')\n",
    "    print(f'Train f1_neg = {train_f1_neg}')\n",
    "    \n",
    "    # Cross-validation\n",
    "    val_loss, val_confusion, val_acc, val_f1_pos, val_f1_neg = eval_epoch_binary(\n",
    "        model, val_data_loader, \n",
    "        loss_fn, optimizer, \n",
    "        device, \n",
    "        test_mode = False\n",
    "    )\n",
    "    \n",
    "    # Record losses to plot learning curve \n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f'Validation loss = {np.mean(val_loss)}', end = ', ')\n",
    "    print(f'Validation accuracy = {val_acc}', end = ', ')\n",
    "    print(f'Validation f1_pos = {val_f1_pos}', end = ', ')\n",
    "    print(f'Validation f1_neg = {val_f1_neg}')\n",
    "\n",
    "    print('-' * 10)\n",
    "    print(f'End epoch {epoch + 1} out of {num_epoch}', end = '\\n\\n')\n",
    "    \n",
    "    # Use accuracy as the metric to select best model \n",
    "    # Store the model with highest validation accuracy \n",
    "    if val_acc > best_acc: \n",
    "        torch.save(model.state_dict(), SESS_PWD + 'best_' + model_name + '.bin')\n",
    "        best_acc = val_acc\n",
    "        \n",
    "        \n",
    "'''Step 3: test the model'''\n",
    "\n",
    "test_loss, test_confusion, test_acc, test_f1_pos, test_f1_neg = eval_epoch_binary(\n",
    "    model, test_data_loader,  \n",
    "    loss_fn, optimizer, \n",
    "    device, \n",
    "    test_mode = True\n",
    ")\n",
    "\n",
    "print(f'Test loss = {np.mean(test_loss)}', end = ', ')\n",
    "print(f'Test accuracy = {test_acc}', end = ', ')\n",
    "print(f'Test f1_pos = {test_f1_pos}', end = ', ')\n",
    "print(f'Test f1_neg = {test_f1_neg}')\n",
    "\n",
    "# Append information in the metric table \n",
    "metric_table.append([model_name, test_confusion, test_acc, test_f1_pos, test_f1_neg])\n",
    "\n",
    "# Plot learning curve \n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_title('Learning curve ' + model_name, fontsize = 15)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.plot(train_losses, label = 'train')\n",
    "ax.plot(val_losses, label = 'validation')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 1 out of 5\n",
      "----------\n",
      "Training batch #928 out of 929\n",
      "Train loss = 0.586885330174018, Train accuracy = 0.7272727272727273, Train f1_pos = 0.8421052631578947, Train f1_neg = 0.0\n",
      "Cross-validating batch #84 out of 85\n",
      "Validation loss = 0.43092338302556205, Validation accuracy = 0.8888888888888888, Validation f1_pos = 0.9411764705882353, Validation f1_neg = 0.0\n",
      "----------\n",
      "End epoch 1 out of 5\n",
      "\n",
      "Start epoch 2 out of 5\n",
      "----------\n",
      "Training batch #928 out of 929\n",
      "Train loss = 0.5866772040035034, Train accuracy = 0.7272727272727273, Train f1_pos = 0.8421052631578947, Train f1_neg = 0.0\n",
      "Cross-validating batch #84 out of 85\n",
      "Validation loss = 0.43090977703823763, Validation accuracy = 0.8888888888888888, Validation f1_pos = 0.9411764705882353, Validation f1_neg = 0.0\n",
      "----------\n",
      "End epoch 2 out of 5\n",
      "\n",
      "Start epoch 3 out of 5\n",
      "----------\n",
      "Training batch #928 out of 929\n",
      "Train loss = 0.5866743089436715, Train accuracy = 0.7272727272727273, Train f1_pos = 0.8421052631578947, Train f1_neg = 0.0\n",
      "Cross-validating batch #84 out of 85\n",
      "Validation loss = 0.4309089744792265, Validation accuracy = 0.8888888888888888, Validation f1_pos = 0.9411764705882353, Validation f1_neg = 0.0\n",
      "----------\n",
      "End epoch 3 out of 5\n",
      "\n",
      "Start epoch 4 out of 5\n",
      "----------\n",
      "Training batch #928 out of 929\n",
      "Train loss = 0.5866740363919697, Train accuracy = 0.7272727272727273, Train f1_pos = 0.8421052631578947, Train f1_neg = 0.0\n",
      "Cross-validating batch #84 out of 85\n",
      "Validation loss = 0.4309088103911456, Validation accuracy = 0.8888888888888888, Validation f1_pos = 0.9411764705882353, Validation f1_neg = 0.0\n",
      "----------\n",
      "End epoch 4 out of 5\n",
      "\n",
      "Start epoch 5 out of 5\n",
      "----------\n",
      "Training batch #928 out of 929\n",
      "Train loss = 0.5866739678049242, Train accuracy = 0.7272727272727273, Train f1_pos = 0.8421052631578947, Train f1_neg = 0.0\n",
      "Cross-validating batch #84 out of 85\n",
      "Validation loss = 0.43090875674696527, Validation accuracy = 0.8888888888888888, Validation f1_pos = 0.9411764705882353, Validation f1_neg = 0.0\n",
      "----------\n",
      "End epoch 5 out of 5\n",
      "\n",
      "\n",
      "# True negative = 0\n",
      "# False positive = 401\n",
      "# False negative = 0\n",
      "# True positive = 9600\n",
      "Test loss = 0.35479526264598954, Test accuracy = 0.9599040095990401, Test f1_pos = 0.9795418601091781, Test f1_neg = 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEYCAYAAABGJWFlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df7xVdZ3v8dc7OMpPFYESwQRv3hIIgbaIQxmmNYCk2SU9lt6rM8VIQ2kzjVrTVHpvc6cZr0M1JjGm1Y0kQ0UzzbQB01sSB0Tih46IOBxROeKIoBCCn/vHWgc3+3zPYQNnnc2B9/Px2A/2Wuv7Xeuzv8B5n/Vjr6WIwMzMrNLbal2AmZkdmBwQZmaW5IAwM7MkB4SZmSU5IMzMLMkBYWZmSQ4I22+Svi7ppVrXsSeSLpEUknrVupYDnaTB+ViFpHGJ5X+XL1ub6DO5jfUuKFvvDklPS/pnSUcU9FFsPzgg7FDyC+A04PVaF9KJbAEuTMy/IF+2L+aT/T2MB/4F+Axw8z6uywrkgLBOTVL3attGRFNEPBoRbxZZU0eQ1K2DNvVzYIqkLmXbfi9wEnDPPq7z5fzv4ZGI+GfgH4D/Jqn//pdr7ckBYR1C0nBJv5C0OX/9TNIxZct7SvoXSU9Kel3SM5JuqDz0kB+a+CtJMyQ1AX8om3+5pL+X1CRpQ97/8LK+ux1iKjskcr6k70naJKlR0jWS3lax3U9IekrSVknzJY3K+16yh8/dXdI/SnpW0h/zz/W/Kz7P9Io+ux2yK6t7TH6IZivwN/m6/jGxzbmSHi6bPjr/fC9K2ibpt5JObavuMncBvYEzyubVA48Az1W5jj1ZnP85uJ3WZ+3EAWGFk/Qu4P8B3YCLgUuAYcDPJSlv1gPoAvwtMBH4O+BDwM8Sq/wbYEC+rs+Xzf9r4FjgIuCfgL8ALq+ixH8kO1wyBfgx8NX8fXP9JWAOsAQ4D7gb+OmeVpp/truAacANwCTga0C/KmpKuZXst/ZJ+Z+3AeeXjSF5+E1qri8PyAeBD5ON28eAJuDB8oBuw2v5tsoPM9XntbSXwfmfL7TjOq09RIRffu3XC/g68FIby/8v8CRwWNm8E4GdwNmt9OkKjAMCeGfZ/AAeS7QP4DcV8+YBj5ZNX5K365VPD86nf1TRbykwp2z6Z8ByQGXzrsz7XtLG5/7TvM05bbQJYHpb41lW9+UV7Ubl88eWzbswH9d35NN/DmwHTqwY26eBf2qjruaxmUwWii8DhwFjgDfIQu46YG2qTxvrXQDcntdwOPABoBFYVD6+fh0YL+9BWEc4C7gTeFNSV0ldgWeAtUCpuZGkiyU9JmkL2Q+hR/JF/7Vifb9oZTu/qpheCQyqor499TsF+HnkP+Fyd1ex3g+RHW+vpm01dvvcEfEY8O9kJ4ybXQAsiIgX8+mzyA7hPFM29gAPUTb2e3Av2d7dn5LtPfw6IvbnqrWPk/39bgN+Q/bv4FMV42sHAAeEdYR+wFVkPxTKXycAxwFIOg/4EfA74BPAWLLfXCE7NFXuRdJeqZjenui7L/2OITssU65yOqUv8HwV7aqV+tw/BT6hzBHABLLDYc36kY1l5dhfSj72exIRfyTbG/skcH7F+vfFv5GF7ijg6Ih4f0T8+36u0wrQdc9NzPbby2R7EDclljX/JvoJYGFEfLZ5gaQPtrK+jv5N8wWg8gqbaq642Uh2rqQtfyQ7dFPu6Fbapj73HLLzNe8HhpD9pn9H2fKXgQay8yCpbVdrDtm5iDfI/i73x39GRMN+rsM6gAPCOsKvgeHA4jYOI3Sn5Q+sTxVaVfUWAR+V9OWy+s+pot+vgSslTY6I1i4JbSS7ZBSA/OqpD1VbWESslLSc7NDSEOCBiNhYUcNHgP+IiA3VrjfhAbJzB09ExKb9WI91Ig4Iay+HSZqSmP8Q2UnX3wO/kHQz2V7DQLIra34QEQvIfgDdIOlvgYVkV+Kc2QF1V+ObZDXNkXQL2Q/0z+TL2vpOxQPA/cBPJF1LdhXUAOD0iPiLvM2dwF9KegxYA3wa2NtvFf+U7GqtI8vqavYj4DJggaTr8m30JTvZ/EJk30PYo4jYQXZ4qRrjEt/TWOu9hs7HAWHtpTfpS1LPiIgFksYC/wuYRba38BzZb7er83bfIzsncTnZ8f8HyI55P1pw3XsUEQ2SLgT+HjiXtw7ZPAC82ka/yM+t/E/gCrLDUuuBn5Q1uwZ4O9nYbCf7ZvFyYDrVm5Nvo/lcQXkN2ySdAVybb+sdwAaywG6vk+eVrk7M+yHZ1VjWicgXDpjtPUkXkV2+e0JEPFPresyK4D0IsypIupFsj+E/gdHAV4BfOBzsYOaAMKtOX+C7+Z8byY77X1nTiswK5kNMZmaW5C/KmZlZ0kF1iKlfv34xePDgWpdhZtZpLF68+KWISH7x86AKiMGDB9PQ4EutzcyqJenZ1pb5EJOZmSU5IMzMLMkBYWZmSQ4IMzNLckCYmVmSA8LMzJIcEGZmlnRQfQ9iX33710+xY+ebICFAAqH8z2waQPmb1PKs5+5t31pW2ZdWt1W+fsr67mrbvP68T8tt7b6drI12q2O37bSxrfLt5KtpUatVp/nv36wIXbuI0e/s0/7rbfc1dkIzH3qa17fvrHUZZmb7pF+vw2n4ylntvl4HBLDy2gm73kcEEdnDfyNi10OAs3nZssrpKO+bL2MPyyNrsPv0rvdv1ZHcduW2WmynbNuJuvMeLT5n+bre2k66bqueh8uKVtelmLMFDogK0luHbnwgxcwOZT5JbWZmSQ4IMzNLckCYmVlSoQEhaYKkJyWtlnR1Yvl4SZskLc1fXy1b9gVJKyQtl3SrpG5F1mpmZrsrLCAkdQFuACYCQ4ELJQ1NNH04Ikbmr2vzvgOBzwOliBgOdAHqi6rVzMxaKnIPYgywOiLWRMR2YA5w7l707wp0l9QV6AGsL6BGMzNrRZEBMRBYVzbdmM+rdJqkxyXdJ2kYQEQ8B1wH/AfwPLApIn6V2oikqZIaJDU0NTW17ycwMzuEFRkQqS8RVH5naAlwfEScDHwHmAcgqQ/Z3sYQ4Figp6SLUhuJiFkRUYqIUv/+yceqmpnZPigyIBqB48qmB1FxmCgiXo2ILfn7e4E6Sf2As4BnIqIpIt4A7gD+pMBazcysQpEBsQg4UdIQSYeRnWS+u7yBpGOU38VM0pi8no1kh5bGSuqRLz8TWFVgrWZmVqGwW21ExA5J04H7ya5CujkiVki6LF8+E5gCTJO0A9gK1EdEAAslzSU7BLUDeAyYVVStZmbWkiIOnluJlUqlaGhoqHUZZmadhqTFEVFKLfM3qc3MLMkBYWZmSQ4IMzNLckCYmVmSA8LMzJIcEGZmluSAMDOzJAeEmZklOSDMzCzJAWFmZkkOCDMzS3JAmJlZkgPCzMySHBBmZpbkgDAzsyQHhJmZJRUaEJImSHpS0mpJVyeWj5e0SdLS/PXVsmVHSZor6QlJqySdVmStZma2u8IeOSqpC3AD8GGgEVgk6e6IWFnR9OGImJxYxbeAX0bElPyZ1j2KqtXMzFoqcg9iDLA6ItZExHZgDnBuNR0lHQGcDnwfICK2R8QrhVVqZmYtFBkQA4F1ZdON+bxKp0l6XNJ9kobl804AmoBbJD0m6SZJPVMbkTRVUoOkhqampnb9AGZmh7IiA0KJeVExvQQ4PiJOBr4DzMvndwVGAzdGxCjgNaDFOQyAiJgVEaWIKPXv3799Kjczs0IDohE4rmx6ELC+vEFEvBoRW/L39wJ1kvrlfRsjYmHedC5ZYJiZWQcpMiAWASdKGpKfZK4H7i5vIOkYScrfj8nr2RgRLwDrJL07b3omUHly28zMClTYVUwRsUPSdOB+oAtwc0SskHRZvnwmMAWYJmkHsBWoj4jmw1CfA2bn4bIGuLSoWs3MrCW99fO48yuVStHQ0FDrMszMOg1JiyOilFrmb1KbmVmSA8LMzJIcEGZmluSAMDOzJAeEmZklOSDMzCzJAWFmZkkOCDMzS3JAmJlZkgPCzMySHBBmZpbkgDAzsyQHhJmZJTkgzMwsyQFhZmZJDggzM0tyQJiZWVKhASFpgqQnJa2WdHVi+XhJmyQtzV9frVjeRdJjku4psk4zM2upsGdSS+oC3AB8GGgEFkm6OyJWVjR9OCImt7Kay4FVwBFF1WlmZmlF7kGMAVZHxJqI2A7MAc6ttrOkQcDZwE0F1WdmZm0oMiAGAuvKphvzeZVOk/S4pPskDSubPwO4EnizrY1ImiqpQVJDU1PTfhdtZmaZIgNCiXlRMb0EOD4iTga+A8wDkDQZ2BARi/e0kYiYFRGliCj1799/f2s2M7NckQHRCBxXNj0IWF/eICJejYgt+ft7gTpJ/YBxwDmS1pIdmvqQpB8XWKuZmVUoMiAWASdKGiLpMKAeuLu8gaRjJCl/PyavZ2NEfCkiBkXE4Lzfv0XERQXWamZmFQq7iikidkiaDtwPdAFujogVki7Ll88EpgDTJO0AtgL1EVF5GMrMzGpAB9PP41KpFA0NDbUuw8ys05C0OCJKqWX+JrWZmSU5IMzMLMkBYWZmSQ4IMzNLckCYmVmSA8LMzJIcEGZmluSAMDOzJAeEmZklOSDMzCzJAWFmZkkOCDMzS3JAmJlZkgPCzMySHBBmZpbkgDAzs6RCA0LSBElPSlot6erE8vGSNklamr++ms8/TtJ8SaskrZB0eZF1mplZS4U9clRSF+AG4MNAI7BI0t0RsbKi6cMRMbli3g7gryNiiaTewGJJDyT6mtlB6o033qCxsZFt27bVupSDQrdu3Rg0aBB1dXVV9yksIIAxwOqIWAMgaQ5wLrDHH/IR8TzwfP5+s6RVwMBq+prZwaGxsZHevXszePBgJNW6nE4tIti4cSONjY0MGTKk6n5FHmIaCKwrm27M51U6TdLjku6TNKxyoaTBwChgYWojkqZKapDU0NTUtP9Vm9kBYdu2bfTt29fh0A4k0bdv373eGysyIFJ/q1ExvQQ4PiJOBr4DzNttBVIv4Hbgioh4NbWRiJgVEaWIKPXv378dyjazA4XDof3sy1gWGRCNwHFl04OA9eUNIuLViNiSv78XqJPUD0BSHVk4zI6IOwqs08yshVdeeYXvfve7e91v0qRJvPLKKwVU1PGKDIhFwImShkg6DKgH7i5vIOkY5bEmaUxez8Z83veBVRFxfYE1mpkltRYQO3fubLPfvffey1FHHVVUWR2qsJPUEbFD0nTgfqALcHNErJB0Wb58JjAFmCZpB7AVqI+IkPR+4GLgD5KW5qv8cr6XYWZWuKuvvpqnn36akSNHUldXR69evRgwYABLly5l5cqVfOxjH2PdunVs27aNyy+/nKlTpwIwePBgGhoa2LJlCxMnTuT9738/v/3tbxk4cCB33XUX3bt3r/Enq54iKk8LdF6lUikaGhpqXYaZtYNVq1Zx0kknAXDNz1ewcn3yNOQ+G3rsEXztoy2ui9ll7dq1TJ48meXLl7NgwQLOPvtsli9fvusqoJdffpmjjz6arVu3csopp/DQQw/Rt2/f3QLiXe96Fw0NDYwcOZLzzz+fc845h4suuqhdP8feKB/TZpIWR0Qp1b7Iy1zNzA4aY8aM2e0S0W9/+9vceeedAKxbt46nnnqKvn377tZnyJAhjBw5EoD3ve99rF27tsPqbQ8OCDM74LX1m35H6dmz5673CxYs4MEHH+R3v/sdPXr0YPz48clLSA8//PBd77t06cLWrVs7pNb2UtVJakmXSzpCme9LWiLpI0UXZ2ZWK71792bz5s3JZZs2baJPnz706NGDJ554gkcffbSDq+sY1e5B/FlEfEvSnwL9gUuBW4BfFVaZmVkN9e3bl3HjxjF8+HC6d+/OO97xjl3LJkyYwMyZMxkxYgTvfve7GTt2bA0rLU61AdH8DYtJwC0R8bj8DRYzO8j95Cc/Sc4//PDDue+++5LLms8z9OvXj+XLl++a/8UvfrHd6ytatd+DWCzpV2QBcX9+A703iyvLzMxqrdo9iD8HRgJrIuJ1SUeTHWYyM7ODVLV7EKcBT0bEK5IuAr4CbCquLDMzq7VqA+JG4HVJJwNXAs8CPyqsKjMzq7lqA2JHZF+5Phf4VkR8C+hdXFlmZlZr1Z6D2CzpS2T3R/pA/rS46h9LZGZmnU61exAXAH8k+z7EC2QP/vmnwqoyM+tkevXqBcD69euZMmVKss348ePZ0/3iZsyYweuvv75rupa3D68qIPJQmA0cKWkysC0ifA7CzKzCsccey9y5c/e5f2VA1PL24dXeauN84PfAJ4DzgYWS0hFpZnYQuOqqq3Z7HsTXv/51rrnmGs4880xGjx7Ne9/7Xu66664W/dauXcvw4cMB2Lp1K/X19YwYMYILLrhgt3sxTZs2jVKpxLBhw/ja174GZDcAXL9+PWeccQZnnHEGkN0+/KWXXgLg+uuvZ/jw4QwfPpwZM2bs2t5JJ53EZz7zGYYNG8ZHPvKRdrvnU7XnIP4WOCUiNgBI6g88COx7TJqZVeu+q+GFP7TvOo95L0z8h1YX19fXc8UVV/DZz34WgNtuu41f/vKXfOELX+CII47gpZdeYuzYsZxzzjmtPs7zxhtvpEePHixbtoxly5YxevToXcu+8Y1vcPTRR7Nz507OPPNMli1bxuc//3muv/565s+fT79+/XZb1+LFi7nllltYuHAhEcGpp57KBz/4Qfr06cNTTz3Frbfeyr/+679y/vnnc/vtt7fLbcWrPQfxtuZwyG3ci75mZp3OqFGj2LBhA+vXr+fxxx+nT58+DBgwgC9/+cuMGDGCs846i+eee44XX3yx1XX85je/2fWDesSIEYwYMWLXsttuu43Ro0czatQoVqxYwcqVK9us55FHHuG8886jZ8+e9OrVi49//OM8/PDDQHG3Fa92D+KXku4Hbs2nLwD2+HQ3SROAb5E9Ue6miPiHiuXjgbuAZ/JZd0TEtdX0NbNDSBu/6RdpypQpzJ07lxdeeIH6+npmz55NU1MTixcvpq6ujsGDBydv810utXfxzDPPcN1117Fo0SL69OnDJZdcssf1tPVwt6JuK17tSeq/AWYBI4CTgVkRcVVbffJLYW8AJgJDgQslDU00fTgiRuava/eyr5lZYerr65kzZw5z585lypQpbNq0ibe//e3U1dUxf/58nn322Tb7n3766cyePRuA5cuXs2zZMgBeffVVevbsyZFHHsmLL764243/WrvN+Omnn868efN4/fXXee2117jzzjv5wAc+0I6ftqWqHxgUEbcDt+/FuscAqyNiDYCkOWRftGt7P2r/+5qZtYthw4axefNmBg4cyIABA/jUpz7FRz/6UUqlEiNHjuQ973lPm/2nTZvGpZdeyogRIxg5ciRjxowB4OSTT2bUqFEMGzaME044gXHjxu3qM3XqVCZOnMiAAQOYP3/+rvmjR4/mkksu2bWOT3/604waNarQp9S1+UxqSZuBVAMBERFHtNF3CjAhIj6dT18MnBoR08vajCcLnUZgPfDFiFhRTd+ydUwFpgK8853vfN+eEt3MOofU85Nt/7TrM6kjYn9up5E6rV8ZNkuA4yNii6RJwDzgxCr7Ntc4i+zwF6VSqfW0MzOzvVLklUiNwHFl04PI9hJ2iYhXI2JL/v5eoE5Sv2r6mplZsYoMiEXAiZKGSDoMqAfuLm8g6ZjmJ9NJGpPXs7GavmZmVqyqT1LvrYjYIWk6cD/Zpao35+cXLsuXzwSmANMk7QC2AvX5XWOTfYuq1cwOTBHR6pfQbO+0db65NW2epO5sSqVS7OlGWGbWOTzzzDP07t2bvn37OiT2U0SwceNGNm/ezJAhQ3Zbts8nqc3MamXQoEE0NjbS1NRU61IOCt26dWPQoEF71ccBYWYHpLq6uha/7VrH8v2UzMwsyQFhZmZJDggzM0tyQJiZWZIDwszMkhwQZmaW5IAwM7MkB4SZmSU5IMzMLMkBYWZmSQ4IMzNLckCYmVmSA8LMzJIcEGZmluSAMDOzpEIDQtIESU9KWi3p6jbanSJpp6QpZfO+IGmFpOWSbpXUrchazcxsd4UFhKQuwA3ARGAocKGkoa20+ybZ86eb5w0EPg+UImI42XOp64uq1czMWipyD2IMsDoi1kTEdmAOcG6i3eeA24ENFfO7At0ldQV6AOsLrNXMzCoUGRADgXVl0435vF3yPYXzgJnl8yPiOeA64D+A54FNEfGr1EYkTZXUIKnBz641M2s/RQaEEvOiYnoGcFVE7Nyto9SHbG9jCHAs0FPSRamNRMSsiChFRKl///7tULaZmUF2GKcojcBxZdODaHmYqATMkQTQD5gkaQdQBzwTEU0Aku4A/gT4cYH1mplZmSIDYhFwoqQhwHNkJ5k/Wd4gIoY0v5f0A+CeiJgn6VRgrKQewFbgTKChwFrNzKxCYQERETskTSe7OqkLcHNErJB0Wb58Zht9F0qaCywBdgCPAbOKqtXMzFpSROVpgc6rVCpFQ4N3NMzMqiVpcUSUUsv8TWozM0tyQJiZWZIDwszMkhwQZmaW5IAwM7MkB4SZmSU5IMzMLMkBYWZmSQ4IMzNLckCYmVmSA8LMzJIcEGZmluSAMDOzJAeEmZklOSDMzCzJAWFmZkmFBoSkCZKelLRa0tVttDtF0k5JU8rmHSVprqQnJK2SdFqRtZqZ2e4KCwhJXYAbgInAUOBCSUNbafdNskeTlvsW8MuIeA9wMrCqqFrNzKylIvcgxgCrI2JNRGwH5gDnJtp9Drgd2NA8Q9IRwOnA9wEiYntEvFJgrWZmVqHIgBgIrCubbszn7SJpIHAeMLOi7wlAE3CLpMck3SSpZ2ojkqZKapDU0NTU1H7Vm5kd4ooMCCXmRcX0DOCqiNhZMb8rMBq4MSJGAa8ByXMYETErIkoRUerfv//+1mxmZrmuBa67ETiubHoQsL6iTQmYIwmgHzBJ0g7gUaAxIhbm7ebSSkCYmVkxigyIRcCJkoYAzwH1wCfLG0TEkOb3kn4A3BMR8/LpdZLeHRFPAmcCKwus1czMKhQWEBGxQ9J0squTugA3R8QKSZflyyvPO1T6HDBb0mHAGuDSomo1M7OWFFF5WqDzKpVK0dDQUOsyzMw6DUmLI6KUWuZvUpuZWZIDwszMkhwQZmaW5IAwM7MkB4SZmSU5IMzMLMkBYWZmSQ4IMzNLckCYmVmSA8LMzJIcEGZmluSAMDOzJAeEmZklOSDMzCzJAWFmZkkOCDMzSyo0ICRNkPSkpNWSWn2mtKRTJO2UNKVifhdJj0m6p8g6zcyspcICQlIX4AZgIjAUuFDS0FbafZPs0aSVLgdWFVWjmZm1rsg9iDHA6ohYExHbgTnAuYl2nwNuBzaUz5Q0CDgbuKnAGs3MrBVFBsRAYF3ZdGM+bxdJA4HzgJmJ/jOAK4E329qIpKmSGiQ1NDU17V/FZma2S5EBocS8qJieAVwVETt36yhNBjZExOI9bSQiZkVEKSJK/fv33/dqzcxsN10LXHcjcFzZ9CBgfUWbEjBHEkA/YJKkHcCpwDmSJgHdgCMk/TgiLiqwXjMzK1NkQCwCTpQ0BHgOqAc+Wd4gIoY0v5f0A+CeiJgHzAO+lM8fD3zR4WBm1rEKC4iI2CFpOtnVSV2AmyNihaTL8uWp8w5mZnaAUETlaYHOq1QqRUNDQ63LMDPrNCQtjohSapm/SW1mZkkOCDMzS3JAmJlZkgPCzMySHBBmZpZU5PcgOo8bx8EbW9tvfUp9iXyfV9aOqzpA6zoUtOvYm1XofjT82X3tvloHBMCAk2HHH9tpZe142XC7XoJ8oNZ1KPB4WcG6HVnIah0QAB/7bq0rMDM74PgchJmZJTkgzMwsyQFhZmZJDggzM0tyQJiZWZIDwszMkhwQZmaW5IAwM7Okg+qBQZKagGf3sXs/4KV2LKe9uK6947r2juvaOwdjXcdHRP/UgoMqIPaHpIbWnqpUS65r77iuveO69s6hVpcPMZmZWZIDwszMkhwQb5lV6wJa4br2juvaO65r7xxSdfkchJmZJXkPwszMkhwQZmaWdEgFhKQJkp6UtFrS1YnlkvTtfPkySaMPkLrGS9okaWn++moH1XWzpA2SlreyvFbjtae6ajVex0maL2mVpBWSLk+06fAxq7KuDh8zSd0k/V7S43ld1yTa1GK8qqmrJv/G8m13kfSYpHsSy9p3vCLikHgBXYCngROAw4DHgaEVbSYB95E9cHkssPAAqWs8cE8Nxux0YDSwvJXlHT5eVdZVq/EaAIzO3/cG/v0A+TdWTV0dPmb5GPTK39cBC4GxB8B4VVNXTf6N5dv+K+Anqe2393gdSnsQY4DVEbEmIrYDc4BzK9qcC/woMo8CR0kacADUVRMR8Rvg5Taa1GK8qqmrJiLi+YhYkr/fDKwCBlY06/Axq7KuDpePwZZ8si5/VV41U4vxqqaumpA0CDgbuKmVJu06XodSQAwE1pVNN9LyP0k1bWpRF8Bp+S7vfZKGFVxTtWoxXtWq6XhJGgyMIvvts1xNx6yNuqAGY5YfLlkKbAAeiIgDYryqqAtq829sBnAl8GYry9t1vA6lgFBiXuVvBdW0aW/VbHMJ2f1STga+A8wruKZq1WK8qlHT8ZLUC7gduCIiXq1cnOjSIWO2h7pqMmYRsTMiRgKDgDGShlc0qcl4VVFXh4+XpMnAhohY3FazxLx9Hq9DKSAagePKpgcB6/ehTYfXFRGvNu/yRsS9QJ2kfgXXVY1ajNce1XK8JNWR/RCeHRF3JJrUZMz2VFet/41FxCvAAmBCxaKa/htrra4ajdc44BxJa8kORX9I0o8r2rTreB1KAbEIOFHSEEmHAfXA3RVt7gb+e34lwFhgU0Q8X+u6JB0jSfn7MWR/bxsLrqsatRivParVeOXb/D6wKiKub6VZh49ZNXXVYswk9Zd0VP6+O3AW8ERFs1qM1x7rqsV4RcSXImJQRAwm+znxbxFxUUWzdh2vrvtebucSETskTQfuJ7ty6OaIWCHpsnz5TOBesqsAVgOvA5ceIHVNAaZJ2gFsBeojv2ShSJJuJbtao5+kRuBrZCfsajZeVdZVk/Ei+w3vYuAP+fFrgC8D7yyrrRZjVk1dtRizAcAPJXUh+wF7W0TcU+xItOwAAAHgSURBVOv/k1XWVat/Yy0UOV6+1YaZmSUdSoeYzMxsLzggzMwsyQFhZmZJDggzM0tyQJiZWZIDwuwAoOzuoC3uzmlWSw4IMzNLckCY7QVJFyl7VsBSSd/Lb+q2RdL/kbRE0q8l9c/bjpT0qLL78t8pqU8+/12SHsxv9LZE0n/JV99L0lxJT0ia3fxNXbNacUCYVUnSScAFwLj8Rm47gU8BPYElETEaeIjsm90APwKuiogRwB/K5s8Gbshv9PYnQPOtEEYBVwBDyZ4PMq7wD2XWhkPmVhtm7eBM4H3AovyX++5kt4N+E/hp3ubHwB2SjgSOioiH8vk/BH4mqTcwMCLuBIiIbQD5+n4fEY359FJgMPBI8R/LLM0BYVY9AT+MiC/tNlP6u4p2bd2/pq3DRn8se78T//+0GvMhJrPq/RqYIuntAJKOlnQ82f+jKXmbTwKPRMQm4D8lfSCffzHwUP4chkZJH8vXcbikHh36Kcyq5N9QzKoUESslfQX4laS3AW8Afwm8BgyTtBjYRHaeAuB/ADPzAFjDW3fWvBj4nqRr83V8ogM/hlnVfDdXs/0kaUtE9Kp1HWbtzYeYzMwsyXsQZmaW5D0IMzNLckCYmVmSA8LMzJIcEGZmluSAMDOzpP8P+AX4n5QSuikAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 172 ms, total: 1min 31s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''Step 1: set-up'''\n",
    "\n",
    "# This is a pathetic way to retrieve the dimensionality of input vector\n",
    "input_size = next(iter(train_data_loader))['input_size'][0].item()\n",
    "num_labels = 2\n",
    "hidden_layers_dim = [100, 50, 70, 20, 5]\n",
    "\n",
    "model = MLPClassifier(input_size, num_labels, hidden_layers_dim, drop_out = 0.2).to(device)\n",
    "\n",
    "model_name = 'MLP'\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 2e-4)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "num_epoch = 5\n",
    "\n",
    "best_acc = 0\n",
    "best_f1_pos = 0\n",
    "best_f1_neg = 0\n",
    "\n",
    "'''Step 2: training loop '''\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epoch): \n",
    "    print(f'Start epoch {epoch + 1} out of {num_epoch}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_confusion, train_acc, train_f1_pos, train_f1_neg = train_epoch_binary(\n",
    "        model, train_data_loader, \n",
    "        loss_fn, optimizer, \n",
    "        device, \n",
    "        clip_grad = True\n",
    "    )\n",
    "    \n",
    "    # Record loss to plot learning curve \n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(f'Train loss = {np.mean(train_loss)}', end = ', ')\n",
    "    print(f'Train accuracy = {train_acc}', end = ', ')\n",
    "    print(f'Train f1_pos = {train_f1_pos}', end = ', ')\n",
    "    print(f'Train f1_neg = {train_f1_neg}')\n",
    "    \n",
    "    # Cross-validation\n",
    "    val_loss, val_confusion, val_acc, val_f1_pos, val_f1_neg = eval_epoch_binary(\n",
    "        model, val_data_loader, \n",
    "        loss_fn, optimizer, \n",
    "        device, \n",
    "        test_mode = False\n",
    "    )\n",
    "    \n",
    "    # Record losses to plot learning curve \n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f'Validation loss = {np.mean(val_loss)}', end = ', ')\n",
    "    print(f'Validation accuracy = {val_acc}', end = ', ')\n",
    "    print(f'Validation f1_pos = {val_f1_pos}', end = ', ')\n",
    "    print(f'Validation f1_neg = {val_f1_neg}')\n",
    "\n",
    "    print('-' * 10)\n",
    "    print(f'End epoch {epoch + 1} out of {num_epoch}', end = '\\n\\n')\n",
    "    \n",
    "    # Use accuracy as the metric to select best model \n",
    "    # Store the model with highest validation accuracy \n",
    "    if val_acc > best_acc: \n",
    "        torch.save(model.state_dict(), SESS_PWD + 'best_' + model_name + '.bin')\n",
    "        best_acc = val_acc\n",
    "        \n",
    "        \n",
    "'''Step 3: test the model'''\n",
    "\n",
    "test_loss, test_confusion, test_acc, test_f1_pos, test_f1_neg = eval_epoch_binary(\n",
    "    model, test_data_loader, \n",
    "    loss_fn, optimizer, \n",
    "    device, \n",
    "    test_mode = True\n",
    ")\n",
    "\n",
    "print(f'Test loss = {np.mean(test_loss)}', end = ', ')\n",
    "print(f'Test accuracy = {test_acc}', end = ', ')\n",
    "print(f'Test f1_pos = {test_f1_pos}', end = ', ')\n",
    "print(f'Test f1_neg = {test_f1_neg}')\n",
    "\n",
    "# Append information in the metric table \n",
    "metric_table.append([model_name, test_confusion, test_acc, test_f1_pos, test_f1_neg])\n",
    "\n",
    "# Plot learning curve \n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_title('Learning curve ' + model_name, fontsize = 15)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.plot(train_losses, label = 'train')\n",
    "ax.plot(val_losses, label = 'validation')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 1 out of 5\n",
      "----------\n",
      "Training batch #928 out of 929\n",
      "Train loss = 0.04748497720570098, Train accuracy = 0.9948148148148148, Train f1_pos = 0.9964435822825736, Train f1_neg = 0.9904335942353087\n",
      "Cross-validating batch #84 out of 85\n",
      "Validation loss = 9.41896083115158, Validation accuracy = 0.1111111111111111, Validation f1_pos = 0.0, Validation f1_neg = 0.2\n",
      "----------\n",
      "End epoch 1 out of 5\n",
      "\n",
      "Start epoch 2 out of 5\n",
      "----------\n",
      "Training batch #928 out of 929\n",
      "Train loss = 7.442119991900385, Train accuracy = 0.2727272727272727, Train f1_pos = 0.0, Train f1_neg = 0.42857142857142855\n",
      "Cross-validating batch #84 out of 85\n",
      "Validation loss = 9.41896083115158, Validation accuracy = 0.1111111111111111, Validation f1_pos = 0.0, Validation f1_neg = 0.2\n",
      "----------\n",
      "End epoch 2 out of 5\n",
      "\n",
      "Start epoch 3 out of 5\n",
      "----------\n",
      "Training batch #928 out of 929\n",
      "Train loss = 7.44053776893307, Train accuracy = 0.2727272727272727, Train f1_pos = 0.0, Train f1_neg = 0.42857142857142855\n",
      "Cross-validating batch #84 out of 85\n",
      "Validation loss = 9.41896083115158, Validation accuracy = 0.1111111111111111, Validation f1_pos = 0.0, Validation f1_neg = 0.2\n",
      "----------\n",
      "End epoch 3 out of 5\n",
      "\n",
      "Start epoch 4 out of 5\n",
      "----------\n",
      "Training batch #928 out of 929\n",
      "Train loss = 7.437411445031579, Train accuracy = 0.2727272727272727, Train f1_pos = 0.0, Train f1_neg = 0.42857142857142855\n",
      "Cross-validating batch #84 out of 85\n",
      "Validation loss = 9.41896083115158, Validation accuracy = 0.1111111111111111, Validation f1_pos = 0.0, Validation f1_neg = 0.2\n",
      "----------\n",
      "End epoch 4 out of 5\n",
      "\n",
      "Start epoch 5 out of 5\n",
      "----------\n",
      "Training batch #928 out of 929\n",
      "Train loss = 7.442809887959913, Train accuracy = 0.2727272727272727, Train f1_pos = 0.0, Train f1_neg = 0.42857142857142855\n",
      "Cross-validating batch #84 out of 85\n",
      "Validation loss = 9.41896083115158, Validation accuracy = 0.1111111111111111, Validation f1_pos = 0.0, Validation f1_neg = 0.2\n",
      "----------\n",
      "End epoch 5 out of 5\n",
      "\n",
      "\n",
      "# True negative = 401\n",
      "# False positive = 0\n",
      "# False negative = 9600\n",
      "# True positive = 0\n",
      "Test loss = 10.23145332138706, Test accuracy = 0.0400959904009599, Test f1_pos = 0.0, Test f1_neg = 0.07710055758507979\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36min, sys: 12min 42s, total: 48min 43s\n",
      "Wall time: 48min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''Step 1: set-up'''\n",
    "\n",
    "# This is a pathetic way to retrieve the dimensionality of input vector\n",
    "input_size = next(iter(train_data_loader))['input_size'][0].item()\n",
    "num_labels = 2\n",
    "hidden_layers_dim = [100, 50, 70, 20, 5]\n",
    "\n",
    "model = BertClassifier(num_labels = 2, drop_out = 0.2).to(device)\n",
    "\n",
    "model_name = 'BERT'\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 2e-4)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps = 0, \n",
    "    num_training_steps = len(train_data_loader)\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "num_epoch = 5\n",
    "\n",
    "best_acc = 0\n",
    "best_f1_pos = 0\n",
    "best_f1_neg = 0\n",
    "\n",
    "'''Step 2: training loop '''\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epoch): \n",
    "    print(f'Start epoch {epoch + 1} out of {num_epoch}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_confusion, train_acc, train_f1_pos, train_f1_neg = train_epoch_binary(\n",
    "        model, train_data_loader, \n",
    "        loss_fn, optimizer, \n",
    "        device, \n",
    "        clip_grad = True, \n",
    "        scheduler = scheduler\n",
    "    )\n",
    "    \n",
    "    # Record loss to plot learning curve \n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    print(f'Train loss = {np.mean(train_loss)}', end = ', ')\n",
    "    print(f'Train accuracy = {train_acc}', end = ', ')\n",
    "    print(f'Train f1_pos = {train_f1_pos}', end = ', ')\n",
    "    print(f'Train f1_neg = {train_f1_neg}')\n",
    "    \n",
    "    # Cross-validation\n",
    "    val_loss, val_confusion, val_acc, val_f1_pos, val_f1_neg = eval_epoch_binary(\n",
    "        model, val_data_loader, \n",
    "        loss_fn, optimizer, \n",
    "        device, \n",
    "        test_mode = False\n",
    "    )\n",
    "    \n",
    "    # Record losses to plot learning curve \n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f'Validation loss = {np.mean(val_loss)}', end = ', ')\n",
    "    print(f'Validation accuracy = {val_acc}', end = ', ')\n",
    "    print(f'Validation f1_pos = {val_f1_pos}', end = ', ')\n",
    "    print(f'Validation f1_neg = {val_f1_neg}')\n",
    "\n",
    "    print('-' * 10)\n",
    "    print(f'End epoch {epoch + 1} out of {num_epoch}', end = '\\n\\n')\n",
    "    \n",
    "    # Use accuracy as the metric to select best model \n",
    "    # Store the model with highest validation accuracy \n",
    "    if val_acc > best_acc: \n",
    "        torch.save(model.state_dict(), SESS_PWD + 'best_' + model_name + '.bin')\n",
    "        best_acc = val_acc\n",
    "        \n",
    "        \n",
    "'''Step 3: test the model'''\n",
    "\n",
    "test_loss, test_confusion, test_acc, test_f1_pos, test_f1_neg = eval_epoch_binary(\n",
    "    model, test_data_loader, \n",
    "    loss_fn, optimizer, \n",
    "    device, \n",
    "    test_mode = True\n",
    ")\n",
    "\n",
    "print(f'Test loss = {np.mean(test_loss)}', end = ', ')\n",
    "print(f'Test accuracy = {test_acc}', end = ', ')\n",
    "print(f'Test f1_pos = {test_f1_pos}', end = ', ')\n",
    "print(f'Test f1_neg = {test_f1_neg}')\n",
    "\n",
    "# Append information in the metric table \n",
    "metric_table.append([model_name, test_confusion, test_acc, test_f1_pos, test_f1_neg])\n",
    "\n",
    "# Plot learning curve \n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_title('Learning curve ' + model_name, fontsize = 15)\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.plot(train_losses, label = 'train')\n",
    "ax.plot(val_losses, label = 'validation')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show final result for comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>test_confusion_matrix</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1_pos</th>\n",
       "      <th>test_f1_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>[[2, 399], [37, 9563]]</td>\n",
       "      <td>0.956404</td>\n",
       "      <td>0.977712</td>\n",
       "      <td>0.009091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP</td>\n",
       "      <td>[[0, 401], [0, 9600]]</td>\n",
       "      <td>0.959904</td>\n",
       "      <td>0.979542</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BERT</td>\n",
       "      <td>[[401, 0], [9600, 0]]</td>\n",
       "      <td>0.040096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model_name   test_confusion_matrix  test_accuracy  test_f1_pos  \\\n",
       "0  logistic_regression  [[2, 399], [37, 9563]]       0.956404     0.977712   \n",
       "1                  MLP   [[0, 401], [0, 9600]]       0.959904     0.979542   \n",
       "2                 BERT   [[401, 0], [9600, 0]]       0.040096     0.000000   \n",
       "\n",
       "   test_f1_neg  \n",
       "0     0.009091  \n",
       "1     0.000000  \n",
       "2     0.077101  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metric_table, columns = colNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeout messages: \n",
    "    \n",
    "1. Giant deep learning model (like BERT) may not work on small dataset; \n",
    "\n",
    "1. Need to explore better upsampling techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
