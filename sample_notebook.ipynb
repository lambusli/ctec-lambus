{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 06\n",
    "\n",
    "Grid search among several models, tokenizers, and parameters. \n",
    "\n",
    "<ul>\n",
    "    <li>Training & validation set: 18386 jigsaw negative + 1614 jigsaw positive + 500 ctec negative + 10000 ctec positive</li>\n",
    "    <li>Test set: 1401 ctec negative + 11600 ctec positive</li>\n",
    "    <li>Tokenizer: BoW, TF-IDF, BERT</li>\n",
    "    <li>Models: Logistic Regression, Multilayer Perceptron, BERT</li>\n",
    "    <li>Optimizer: AdamW (I notice that using Adam will make the predictions very biased)</li>\n",
    "    <li>Normalization of dataset (mean 0 sd 1 in training set) will be performed</li>\n",
    "</ul>\n",
    "\n",
    "## Part 1: Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 0 --> testing mode \n",
    "# 1 --> development mode \n",
    "toy_mode = 0\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "from math import inf\n",
    "import itertools as it\n",
    "from time import time\n",
    "\n",
    "# Define constants \n",
    "NUM_LABELS = 2\n",
    "MAX_LEN = 300    # Max text length for encoding purpose \n",
    "BATCH_SIZE = 32\n",
    "PRETRAINED_BERT_NAME = 'bert-base-uncased'\n",
    "DATA_PWD = './'\n",
    "SESS_PWD = './xprmt_05a_another/'\n",
    "LOG_NAME = './xprmt_06.log'\n",
    "\n",
    "# Enable GPU if possible \n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data \\& train-val-test split\n",
    "\n",
    "`toy_mode = 1` means we are in development mode and will only load very little number of dataset, so that we can debug without wasting time wating for results. \n",
    "\n",
    "`toy_mode = 0` means we are in testing mode and will load all data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load jigsaw and ctec datasets \n",
    "def load_data(toxic_threshold): \n",
    "    '''\n",
    "    Load dataset (mix of ctec and jigsaw) and return train, val, test sets \n",
    "    @Params\n",
    "    -- toxic_threshold: Float. If the toxic score of a text is above the threshold, then we classify the text as toxic. \n",
    "    @Return \n",
    "    (X_train_text, X_val_text, X_test_text, y_train, y_val, y_test)\n",
    "    '''\n",
    "    \n",
    "\n",
    "    jigsaw_df = pd.read_csv(DATA_PWD + 'train_preproc_shrk.csv')\n",
    "    ctec_df = pd.read_csv(DATA_PWD + 'ctec_training_data_preproc.csv')\n",
    "\n",
    "    # target >= threshold --> toxic --> label = 1\n",
    "    # target < threshold --> non-toxic --> label = 0\n",
    "    jigsaw_df.loc[jigsaw_df['target'] >= toxic_threshold, 'label'] = 1\n",
    "    jigsaw_df.loc[jigsaw_df['target'] < toxic_threshold, 'label'] = 0\n",
    "\n",
    "    # Split by label \n",
    "    jigsaw_neg = jigsaw_df[jigsaw_df['label'] == 0]\n",
    "    jigsaw_pos = jigsaw_df[jigsaw_df['label'] == 1]\n",
    "    ctec_neg = ctec_df[ctec_df['label'] == 0]\n",
    "    ctec_pos = ctec_df[ctec_df['label'] == 1]\n",
    "\n",
    "    # Show number of positive and negative examples in each dataset \n",
    "#     print(f'jigsaw # negative examples = {jigsaw_neg.shape[0]}')\n",
    "#     print(f'jigsaw # positive examples = {jigsaw_pos.shape[0]}')\n",
    "#     print(f'ctec # negative examples = {ctec_neg.shape[0]}')\n",
    "#     print(f'ctec # positive examples = {ctec_pos.shape[0]}')\n",
    "\n",
    "    # Create training set and test set based on the scheme described above\n",
    "\n",
    "    # Randomly sampling indices \n",
    "    indpos = np.random.choice(range(ctec_pos.shape[0]), size = 10000, replace = False)\n",
    "    indneg = np.random.choice(range(ctec_neg.shape[0]), size = 500, replace = False)\n",
    "    notindpos = np.setdiff1d(range(ctec_pos.shape[0]), indpos)\n",
    "    notindneg = np.setdiff1d(range(ctec_neg.shape[0]), indneg)\n",
    "\n",
    "    # Combo of training and validation set \n",
    "    df_trainval = jigsaw_df.append(\n",
    "        ctec_pos.iloc[indpos], \n",
    "        ignore_index = True\n",
    "    ).append(\n",
    "        ctec_neg.iloc[indneg], \n",
    "        ignore_index = True\n",
    "    )\n",
    "\n",
    "    # Test set\n",
    "    df_test = ctec_neg.iloc[notindneg].append(\n",
    "        ctec_pos.iloc[notindpos], \n",
    "        ignore_index = True\n",
    "    )\n",
    "\n",
    "\n",
    "    # Split in to training set and validation set \n",
    "    df_train, df_val = train_test_split(\n",
    "        df_trainval, \n",
    "        test_size = 0.1, \n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "#     print(f'# positive in training set = {(df_train.label == 1).sum()}')\n",
    "#     print(f'# negative in training set = {(df_train.label == 0).sum()}')\n",
    "#     print(f'# positive in validation set = {(df_val.label == 1).sum()}')\n",
    "#     print(f'# negative in validation set = {(df_val.label == 0).sum()}')\n",
    "#     print(f'# positive in test set = {(df_test.label == 1).sum()}')\n",
    "#     print(f'# negative in test set = {(df_test.label == 0).sum()}')\n",
    "    \n",
    "    # Extract texts and labels from dataframes \n",
    "    X_train_text = df_train['comment_text'].tolist()\n",
    "    y_train = df_train['label'].astype(int).tolist()\n",
    "    X_val_text = df_val['comment_text'].tolist()\n",
    "    y_val = df_val['label'].astype(int).tolist()\n",
    "    X_test_text = df_test['comment_text'].tolist()\n",
    "    y_test = df_test['label'].astype(int).tolist()\n",
    "\n",
    "    if toy_mode: \n",
    "        X_train_text = X_train_text[:50]\n",
    "        y_train = y_train[:50]\n",
    "        X_val_text = X_val_text[:30]\n",
    "        y_val = y_val[:30]\n",
    "        X_test_text = X_test_text[:20]\n",
    "        y_test = y_test[:20]\n",
    "    \n",
    "    return X_train_text, X_val_text, X_test_text, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text encoding \\& creating PyTorch `Dataset`, `DataLoader`\n",
    "\n",
    "For convenience of creating PyTorch `DataSet`, we will will fit `CountVectorizer()` and `TfidfVectorizer` for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_text, X_val_text, X_test_text are the same regardless of what threshold we choose \n",
    "# It is convenient to initialize vectorizers now. \n",
    "X_train_text, X_val_text, X_test_text, y_train, y_val, y_test = load_data(toxic_threshold = .5)\n",
    "\n",
    "countVect = CountVectorizer()\n",
    "countVect.fit(X_train_text)\n",
    "\n",
    "tfidfVect = TfidfVectorizer()\n",
    "tfidfVect.fit(X_train_text)\n",
    "\n",
    "bertTokenizer = BertTokenizer.from_pretrained(PRETRAINED_BERT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize PyTorch and GPU computation, we create instances of `Dataset` instead of using our original dataset. \n",
    "\n",
    "<b>Notice. </b> Text encoding is performed inside `__getitem()__` function of `Dataset` class. \n",
    "\n",
    "`DataLoader` provides a way to iterate through a dataset with given batch size. \n",
    "\n",
    "<b style=\"color:red\">Attention! </b> All first-initialized PyTorch tensors must be moved to GPU memory manually! Pay attention when you initialize a PyTorch tensor! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicDataset(Dataset): \n",
    "    def __init__(self, texts, labels, tokenizer, max_len = MAX_LEN): \n",
    "        '''\n",
    "        Instantiate ToxicDataset() class \n",
    "        @Params\n",
    "        -- texts: comments texts of the dataset (input)\n",
    "        -- labels: labels corresponding to the texts \n",
    "        -- tokenizer: {'bow', 'tfidf', 'bert'} the scheme of encoding texts into numerical data\n",
    "        -- max_len: if we use BERT tokenizer, we represent texts with `max_len`-dim vectors\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.texts = texts \n",
    "        self.labels = labels \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len \n",
    "        \n",
    "    def __len__(self): \n",
    "        '''Return the size of dataset '''\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx, mean = None, sd = None): \n",
    "        '''\n",
    "        This method must be overwritten by programmer \n",
    "        mean and sd are for normalization purposes\n",
    "        '''\n",
    "        \n",
    "        text = str(self.texts[idx])\n",
    "        # Vector embedding of text. Will be computed by tokenizer \n",
    "        input_vec = None\n",
    "        # Must be PyTorch tensor \n",
    "        label = torch.tensor(\n",
    "            self.labels[idx], \n",
    "            dtype = torch.long\n",
    "        ).to(device)\n",
    "        # Only applicable to BERT\n",
    "        attention_mask = torch.tensor([-1])\n",
    "        \n",
    "        '''\n",
    "        Encode text data according to the given tokenizer\n",
    "        The result of encoding must be PyTorch Tensor. \n",
    "        '''\n",
    "        # BoW encoding \n",
    "        if self.tokenizer == 'bow': \n",
    "            input_vec = torch.Tensor(\n",
    "                countVect.transform([text]).toarray()\n",
    "            ).to(device).flatten()\n",
    "            \n",
    "        # TF-IDF encoding\n",
    "        elif self.tokenizer == 'tfidf': \n",
    "            input_vec = torch.Tensor(\n",
    "                tfidfVect.transform([text]).toarray()\n",
    "            ).to(device).flatten()\n",
    "            \n",
    "        # BERT encoding \n",
    "        elif self.tokenizer == 'bert': \n",
    "            encoding = bertTokenizer.encode_plus(\n",
    "                text, \n",
    "                add_special_tokens = True, \n",
    "                truncation = True, \n",
    "                max_length = self.max_len, \n",
    "                return_token_type_ids = False, \n",
    "                pad_to_max_length = True, \n",
    "                return_attention_mask = True, \n",
    "                return_tensors = 'pt'\n",
    "            )\n",
    "            \n",
    "            input_vec = encoding['input_ids'].to(device).flatten()\n",
    "            attention_mask = encoding['attention_mask'].to(device).flatten()\n",
    "            \n",
    "        if not(mean is None) and not(sd is None): \n",
    "            input_vec = (input_vec - mean) / sd\n",
    "            \n",
    "        return {\n",
    "            'text': text, \n",
    "            'input_vec': input_vec, \n",
    "            'input_size': input_vec.shape[0], \n",
    "            'attention_mask': attention_mask, \n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(\n",
    "    texts, labels, \n",
    "    tokenizer, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    max_len = MAX_LEN\n",
    "): \n",
    "    '''\n",
    "    Helper function for creating DataLoader.  \n",
    "    @Params\n",
    "    -- texts: comments texts of the dataset (input)\n",
    "    -- labels: labels corresponding to the texts \n",
    "    -- tokenizer: {'bow', 'tfidf', 'bert'} the scheme of encoding texts into numerical data\n",
    "    -- batch_size: (as the name suggests)\n",
    "    -- max_len: if we use BERT tokenizer, we represent texts with `max_len`-dim vectors\n",
    "    '''\n",
    "    \n",
    "    dataset = ToxicDataset(\n",
    "        texts = texts, \n",
    "        labels = labels, \n",
    "        tokenizer = tokenizer, \n",
    "        max_len = max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size, \n",
    "        num_workers = 0    # We can change this value to enable multiprocessing\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "<b style=\"color:red;\">Attention!</b> I actually did not use normalization in this experiment, because some initial trial indicates that normalization actually messes up the models for some unknown reasons. \n",
    "\n",
    "Usually, normalizing the training set to mean 0 and sd 1 will improve the performance. Here we define a helper function to return the mean vector and sd vector of training set, so that we can apply those two vectors to validation set and test set. This helper function also returns the dimensionality of input vectors, which will come in handy later. \n",
    "\n",
    "I choose not to use `.mean()` and `.sd()` function to save space by not loading all data at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_sd_ndim(data_loader): \n",
    "    '''\n",
    "    Given the torch dataloader of a dataset, return the a tuple of 1. vector of all means; 2. vector of all standard deviations; 3. dimensionality of input\n",
    "    '''\n",
    "    print(f'\\rAcquiring mean and standard deviation....', end = '')\n",
    "    \n",
    "    sum_for_mean = None\n",
    "    sum_for_sqmean = None\n",
    "    ndim = 0\n",
    "    counter = 0\n",
    "    \n",
    "    for batch in data_loader: \n",
    "        # Initialize vector `summation` with correct dimension\n",
    "        if sum_for_mean is None or sum_for_sqmean is None:             \n",
    "            ndim = batch['input_size'][0].item()\n",
    "            sum_for_mean = torch.zeros(ndim).to(device)\n",
    "            sum_for_sqmean = torch.zeros(ndim).to(device)\n",
    "            \n",
    "        # Batch of input vectors \n",
    "        input_vec = batch['input_vec'].float()\n",
    "        \n",
    "        # Average of batch\n",
    "        sum_for_mean += torch.sum(input_vec, axis = 0)\n",
    "        \n",
    "        # Average of square of batch (for calculating sd)\n",
    "        sum_for_sqmean += torch.sum(input_vec ** 2, axis = 0)\n",
    "        \n",
    "        counter += len(batch)\n",
    "        \n",
    "    mean_vec = sum_for_mean / counter\n",
    "    sd_vec = torch.sqrt((sum_for_sqmean / counter) - (sum_for_mean / counter) ** 2)\n",
    "    \n",
    "    return mean_vec, sd_vec, ndim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Models and model-specific parameters\n",
    "\n",
    "### Create model class\n",
    "\n",
    "We define our models / classifiers by inherting and overwriting `nn.Module` class. \n",
    "\n",
    "<b>Model-specific hyperparameters</b> (<span style=\"color:blue;\">hidden layers, drop-out rate, etc.<span/>) should be defined as attributes of each model class. \n",
    "    \n",
    "<b style=\"color:red\">Attention! </b> `__init__()` must use clases in `torch.nn` module specify inter-layer operations that (1) have parameters to train or (2) change dimensionality (e.g. linear map, convolution, dropout); on the other hand, all operations within layer without trained parameters or change in dimensionality can either be treated as separate layter using `torch.nn` or as the same layer using `torch.nn.Functional`. \n",
    "\n",
    "#### BERT classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):     \n",
    "    def __init__(self, num_labels, drop_out = 0.2):   \n",
    "        '''\n",
    "        @Params\n",
    "        -- drop_out: Drop out rate. By default = 0.2\n",
    "        '''\n",
    "        \n",
    "        # Must instantiate the parent class \n",
    "        super(BertClassifier, self).__init__()\n",
    "        \n",
    "        # pretrained BERT model\n",
    "        self.bert = BertModel.from_pretrained(PRETRAINED_BERT_NAME)\n",
    "        \n",
    "        # Dropout rate \n",
    "        self.drop = nn.Dropout(p = drop_out)\n",
    "        \n",
    "        # Operation from BERT's last hidden layer to output layer\n",
    "        # A linear map to a 2-dimensional vector (2 === binary classification)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_vec, **kwargs): \n",
    "        '''\n",
    "        Define the feed-forward function of the model\n",
    "        @kwargs\n",
    "        -- attention_mask: the attention mask for BERT\n",
    "        '''\n",
    "        # BERT: from input layer to the last hidden layer \n",
    "        _, pooled_output = self.bert(\n",
    "            input_ids = input_vec, \n",
    "            attention_mask = kwargs['attention_mask']\n",
    "        )\n",
    "        \n",
    "        # Drop out neurons according to dropout rate \n",
    "        output = self.drop(pooled_output)\n",
    "        \n",
    "        # Return the last layer \n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer perceptron classifier\n",
    "\n",
    "i.e. the naive deep neural network\n",
    "\n",
    "<b style=\"color:red;\">Attention!</b> To duplicate a PyTorch tensor, we must use `.clone()` function so that the original tensor will not be accidentally overwritten. \n",
    "\n",
    "<b>Choice of coding style</b>. MLP can be written with either `nn.Module` or `nn.Sequential`. For now we choose the former because `nn.Sequential` does not have well-defined methods (such as `.add()` in TensorFlow). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module): \n",
    "    def __init__(self, input_size, num_labels, hidden_layers_dim, drop_out = 0.2): \n",
    "        '''\n",
    "        @Params\n",
    "        -- input_size: the dimensionality of input vector \n",
    "        -- num_labels: the number of classes \n",
    "        -- hidden_layers_dim: the list of dimensionalities for each hidden layer\n",
    "        '''\n",
    "        \n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        curr_layer_dim = input_size\n",
    "        self.hidden_layers = nn.ModuleList([])\n",
    "        \n",
    "        for dim in hidden_layers_dim: \n",
    "            self.hidden_layers.append(nn.Linear(curr_layer_dim, dim))\n",
    "            self.hidden_layers.append(nn.ReLU())\n",
    "            curr_layer_dim = dim\n",
    "            \n",
    "        self.exit_layer = nn.Linear(curr_layer_dim, num_labels)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_vec, **kwargs): \n",
    "        '''\n",
    "        Define the feed forward function\n",
    "        '''\n",
    "        vec = input_vec.clone()\n",
    "        \n",
    "        # Hidden layers\n",
    "        for layer in self.hidden_layers: \n",
    "            vec = layer(vec)\n",
    "            \n",
    "        # Exit layer\n",
    "        return F.softmax(self.exit_layer(vec), dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression \n",
    "\n",
    "Logistic regression is nothing but MLP with no hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier(MLPClassifier): \n",
    "    def __init__(self, input_size, num_labels = 2): \n",
    "        super().__init__(\n",
    "            input_size, \n",
    "            num_labels, \n",
    "            hidden_layers_dim = [], \n",
    "            drop_out = 0\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Helper functions for training an epoch and evaluating model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Choice of coding style</b>. Unlike TensorFlow and scikit-learn, most PyTorch code I have seen defines the training loops outside their corresponding model classes. This coding style yields better flexibility and controlling power while sacrificing compactness and reusability of code. In other words, it is not common to call something like `model.fit()` in PyTorch. \n",
    "\n",
    "I am not sure whether writing the training functions inside model classes will cause problems, because the optimizer depends on `model.parameters()`. \n",
    "\n",
    "For now, we will write training functions outside model classes. \n",
    "\n",
    "Consequently, we tune the <b>non-model-specific parameters</b> (<span style=\"color:blue;\">optimizer, loss function, number of iteration, number of epoch, etc.</span>) outside model classes. However, these parameters can sometimes be model specific and we need to make ad hoc adjustments to our code. \n",
    "\n",
    "<b style=\"color:red;\">Warning!</b> Code for computing metrics is only applicable to binary classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_binary(\n",
    "    model, data_loader, \n",
    "    loss_fn, optimizer, \n",
    "    device, \n",
    "    scheduler = None, \n",
    "    clip_grad = False  # Enable gradient clipping? \n",
    "): \n",
    "    '''\n",
    "    The helper function that trains one epoch \n",
    "    !! Code for computing metrics is only applicable to binary classification\n",
    "    '''\n",
    "    # Set the model to training mode \n",
    "    model = model.train()\n",
    "    # Clean GPU cache \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Record loss of training on each batch \n",
    "    losses = []\n",
    "    # label, pred_class \n",
    "    # index 00 --> true negative \n",
    "    # index 01 --> false postive \n",
    "    # index 10 --> false negative \n",
    "    # index 11 --> true positive \n",
    "    cat_count = [0, 0, 0, 0]\n",
    "    batch_counter = 0\n",
    "    \n",
    "    # Train each batch \n",
    "    for batch in data_loader: \n",
    "        print(f'\\rTraining batch #{batch_counter} out of {len(data_loader)}', end = '')\n",
    "        \n",
    "        # Load data from current batch \n",
    "        input_vec = batch['input_vec'].to(device)\n",
    "        if not isinstance(model, BertClassifier): \n",
    "            # different models require different datatypes \n",
    "            input_vec = input_vec.float()\n",
    "        label = batch['label'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        # Forward pass \n",
    "        output = None\n",
    "        if isinstance(model, BertClassifier): \n",
    "            '''Need to pass attention mask if model is BERT'''\n",
    "            output = model.forward(input_vec = input_vec, attention_mask = attention_mask)\n",
    "        else: \n",
    "            '''Otherwise, only input_vec is required argument'''\n",
    "            output = model.forward(input_vec = input_vec)\n",
    "            \n",
    "        # Probability of predicted class and predicted class \n",
    "        # torch.max with dim==1 returns (maxval, argmax)\n",
    "        pred_prob, pred_class = torch.max(output, dim = 1)\n",
    "            \n",
    "        # Compute loss \n",
    "        loss = loss_fn(output, label)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Backprop \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping \n",
    "        if clip_grad: \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.)\n",
    "            \n",
    "        # Update optimizer and scheduler \n",
    "        # scheduler is only used for BERT\n",
    "        optimizer.step()\n",
    "        if scheduler: \n",
    "            scheduler.step()\n",
    "        \n",
    "        # Compute which category each examples falls in\n",
    "        # 00 --> true negative \n",
    "        # 01 --> false positive \n",
    "        # 10 --> false negative \n",
    "        # 11 --> true positive \n",
    "        cats = 2 * label  + pred_class\n",
    "        # Count each category \n",
    "        for i in range(4): \n",
    "            cat_count[i] += (cats == i).sum().item()\n",
    "            \n",
    "        # Post-processing \n",
    "        # Failure to do so (especially clearing optimizer) will result in unexpected problems \n",
    "        loss.detach() # delete computational history \n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        batch_counter += 1\n",
    "            \n",
    "    '''Metrics of epoch'''\n",
    "    # Train accuracy of epoch\n",
    "    acc = (cat_count[0] + cat_count[3]) / np.sum(cat_count)\n",
    "    # Confusion matrix \n",
    "    confusion = np.array([[cat_count[0], cat_count[1]], [cat_count[2], cat_count[3]]])\n",
    "    # F1 score assuming positive example is scarce\n",
    "    try:\n",
    "        f1_pos = cat_count[3] / (cat_count[3] + .5 * cat_count[1] + .5 * cat_count[2])\n",
    "    except ZeroDivisionError: \n",
    "        f1_pos = inf\n",
    "    # F1 score assuming negative example is scarce \n",
    "    try:\n",
    "        f1_neg = cat_count[0] / (cat_count[0] + .5 * cat_count[1] + .5 * cat_count[2])\n",
    "    except ZeroDivisionError: \n",
    "        f1_neg = inf\n",
    "    \n",
    "    return np.mean(losses), confusion, acc, f1_pos, f1_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch_binary(\n",
    "    model, data_loader, \n",
    "    loss_fn, optimizer, \n",
    "    device, \n",
    "    scheduler = None, \n",
    "    test_mode = False\n",
    "): \n",
    "    '''\n",
    "    The helper function that evaluates the model\n",
    "    Runs only forward pass without backprop\n",
    "    Primarily used for cross-validation\n",
    "    !! Code for computing metrics is only applicable to binary classification\n",
    "    '''\n",
    "    \n",
    "    # Set the model to training mode \n",
    "    model = model.eval()\n",
    "    # Clean GPU cache \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Record loss of training on each batch \n",
    "    losses = []\n",
    "    # label, pred_class \n",
    "    # index 00 --> true negative \n",
    "    # index 01 --> false postive \n",
    "    # index 10 --> false negative \n",
    "    # index 11 --> true positive \n",
    "    cat_count = [0, 0, 0, 0]\n",
    "    batch_counter = 0\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for batch in data_loader: \n",
    "            if not test_mode: \n",
    "                print(f'\\rCross-validating batch #{batch_counter} out of {len(data_loader)}', end = '')\n",
    "\n",
    "            # Load data from current batch\n",
    "            input_vec = batch['input_vec'].to(device)\n",
    "            if not isinstance(model, BertClassifier): \n",
    "                # different models require different datatypes \n",
    "                input_vec = input_vec.float()\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass \n",
    "            output = None\n",
    "            if isinstance(model, BertClassifier): \n",
    "                '''Need to pass attention mask if model is BERT'''\n",
    "                output = model.forward(input_vec = input_vec, attention_mask = attention_mask)\n",
    "            else: \n",
    "                '''Otherwise, only input_vec is required argument'''\n",
    "                output = model.forward(input_vec = input_vec)\n",
    "\n",
    "            # torch.max with dim=1 returns (maxvals, indices)\n",
    "            # indices are the labels we want to predict \n",
    "            preds_prob, preds_class = torch.max(output, dim = 1)\n",
    "\n",
    "            # Compute which category each examples falls in\n",
    "            # 00 --> true negative \n",
    "            # 01 --> false positive \n",
    "            # 10 --> false negative \n",
    "            # 11 --> true positive \n",
    "            # Debug\n",
    "            cats = 2 * label  + preds_class\n",
    "            # Count each category \n",
    "            for i in range(4): \n",
    "                cat_count[i] += (cats == i).sum().item()\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(output, label)\n",
    "\n",
    "            # For analysis purpose \n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            batch_counter += 1\n",
    "            \n",
    "    # Train accuracy of epoch\n",
    "    acc = (cat_count[0] + cat_count[3]) / np.sum(cat_count)\n",
    "    # Confusion matrix \n",
    "    confusion = np.array([[cat_count[0], cat_count[1]], [cat_count[2], cat_count[3]]])\n",
    "    # F1 score assuming positive example is scarce\n",
    "    try:\n",
    "        f1_pos = cat_count[3] / (cat_count[3] + .5 * cat_count[1] + .5 * cat_count[2])\n",
    "    except ZeroDivisionError: \n",
    "        f1_pos = inf\n",
    "    # F1 score assuming negative example is scarce \n",
    "    try:\n",
    "        f1_neg = cat_count[0] / (cat_count[0] + .5 * cat_count[1] + .5 * cat_count[2])\n",
    "    except ZeroDivisionError: \n",
    "        f1_neg = inf\n",
    "        \n",
    "    return np.mean(losses), confusion, acc, f1_pos, f1_neg \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Grid search and training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a metric table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table head of final results \n",
    "colNames = ['model_name', 'encoder', 'toxic_threshold', 'test_confusion_matrix', 'test_accuracy', 'test_f1_pos', 'test_f1_neg']\n",
    "\n",
    "# Array for storing final results \n",
    "metric_table = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of hyperparameters for grid search\n",
    "\n",
    "We start from simple GridSearch where the type of hyperparameters to search are consistent across different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar format as in sklearn\n",
    "\n",
    "gridsearch = [\n",
    "    ('mlp', {\n",
    "        'tokenizers': ['bow', 'tfidf', 'bert'], \n",
    "        'toxic_thresholds': [0.15, 0.3, 0.45]\n",
    "    }), \n",
    "    ('bert', {\n",
    "        'tokenizers': ['bert'], \n",
    "        'toxic_thresholds': [0.15, 0.3, 0.45]\n",
    "    }), \n",
    "    ('logistic_regression', {\n",
    "        'tokenizers': ['bow', 'tfidf', 'bert'], \n",
    "        'toxic_thresholds': [0.15, 0.3, 0.45]\n",
    "    })\n",
    "]\n",
    "\n",
    "\n",
    "def init_model(model_name, input_size, num_labels, **kwargs): \n",
    "    '''\n",
    "    Helper function for instantiating the model\n",
    "    @Params\n",
    "    -- input_size: the dimensionality of input vector\n",
    "    -- num_labels: number of output labels \n",
    "    @kwargs\n",
    "    -- hidden_layers_dim: list of hidden layers for MLP\n",
    "    '''\n",
    "    if model_name == 'logistic_regression': \n",
    "        return LogisticRegressionClassifier(input_size).to(device)\n",
    "    \n",
    "    elif model_name == 'mlp': \n",
    "        return MLPClassifier(input_size, num_labels, kwargs['hidden_layers_dim']).to(device)\n",
    "    \n",
    "    elif model_name == 'bert': \n",
    "        return BertClassifier(num_labels = num_labels).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters that are not part of grid search\n",
    "\n",
    "Intuitive insight from xprmt_05a_another indicate that 4 epochs are a good amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers_dim = [100, 50, 70, 20, 5]\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "num_epoch = 4\n",
    "\n",
    "# optimizer and scheduler needs to be inside gridsearch loop, because parameters of those models are based on specific models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-search loop\n",
    "\n",
    "Since we are doing grid search automatically, we will no longer plot learning curve. \n",
    "\n",
    "Also, we will no longer store the binary file of trained models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4h 57min 16s, sys: 40min 10s, total: 5h 37min 27s\n",
      "Wall time: 2h 43min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logfile = open(LOG_NAME, 'w')\n",
    "\n",
    "for grid in gridsearch: \n",
    "    model_name = grid[0]\n",
    "    hyperparams = grid[1]\n",
    "    \n",
    "    for tokenizer, toxic_threshold in it.product(hyperparams['tokenizers'], hyperparams['toxic_thresholds']): \n",
    "        \n",
    "        logfile.write('=' * 20 + 'start of model' + '=' * 20 + '\\n')\n",
    "        logfile.write(f'{model_name}, {tokenizer} tokenizer, toxic_threshold = {toxic_threshold}\\n\\n')\n",
    "        logfile.flush()\n",
    "        \n",
    "        '''Step 1: prepare data and hyperparameters'''\n",
    "        \n",
    "        # Load data according to toxic_threshold \n",
    "        X_train_text, X_val_text, X_test_text, y_train, y_val, y_test = load_data(toxic_threshold)\n",
    "        \n",
    "        # Create DataLoader\n",
    "        train_data_loader = create_data_loader(\n",
    "            X_train_text, y_train, tokenizer\n",
    "        )\n",
    "\n",
    "        val_data_loader = create_data_loader(\n",
    "            X_val_text, y_val, tokenizer\n",
    "        )\n",
    "\n",
    "        test_data_loader = create_data_loader(\n",
    "            X_test_text, y_test, tokenizer\n",
    "        )\n",
    "        \n",
    "        # get mean, sd, and input dimension\n",
    "        # mean_vec, sd_vec, ndim = get_mean_sd_ndim(train_data_loader)\n",
    "        ndim = next(iter(train_data_loader))['input_size'][0].item()\n",
    "        \n",
    "        # Instantiate the model \n",
    "        model = init_model(\n",
    "            model_name, \n",
    "            input_size = ndim, \n",
    "            num_labels = NUM_LABELS, \n",
    "            hidden_layers_dim = hidden_layers_dim\n",
    "        )\n",
    "        \n",
    "        # Define optimizer \n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "        \n",
    "        # Some model dependent parameters\n",
    "        if model_name == 'logistic_regression': \n",
    "            clip_grad = False\n",
    "            scheduler = None\n",
    "        elif model_name == 'mlp': \n",
    "            clip_grad = True\n",
    "            scheduler = None\n",
    "        elif model_name == 'bert': \n",
    "            clip_grad = True \n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, \n",
    "                num_warmup_steps = 0, \n",
    "                num_training_steps = len(train_data_loader)\n",
    "            )\n",
    "        \n",
    "        '''Step 2: training loop'''\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for epoch in range(num_epoch): \n",
    "            logfile.write(f'Start epoch {epoch + 1} out of {num_epoch}\\n')\n",
    "            logfile.write('-' * 10 + '\\n')\n",
    "            logfile.flush()\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_confusion, train_acc, train_f1_pos, train_f1_neg = train_epoch_binary(\n",
    "                model, train_data_loader, \n",
    "                loss_fn, optimizer, \n",
    "                device, \n",
    "                clip_grad = clip_grad, \n",
    "                scheduler = scheduler\n",
    "            )\n",
    "            \n",
    "            # Record loss to plot learning curve \n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            logfile.write(f'Train loss = {np.mean(train_loss)}, ')\n",
    "            logfile.write(f'Train accuracy = {train_acc}, ')\n",
    "            logfile.write(f'Train f1_pos = {train_f1_pos}, ')\n",
    "            logfile.write(f'Train f1_neg = {train_f1_neg}\\n')\n",
    "            logfile.flush()\n",
    "            \n",
    "            # Cross-validation\n",
    "            val_loss, val_confusion, val_acc, val_f1_pos, val_f1_neg = eval_epoch_binary(\n",
    "                model, val_data_loader, \n",
    "                loss_fn, optimizer, \n",
    "                device\n",
    "            )\n",
    "            \n",
    "            # Record losses to plot learning curve \n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            logfile.write(f'Validation loss = {np.mean(val_loss)}, ')\n",
    "            logfile.write(f'Validation accuracy = {val_acc}, ')\n",
    "            logfile.write(f'Validation f1_pos = {val_f1_pos}, ')\n",
    "            logfile.write(f'Validation f1_neg = {val_f1_neg}\\n')\n",
    "            \n",
    "            logfile.write('-' * 10 + '\\n')\n",
    "            logfile.write(f'End epoch {epoch + 1} out of {num_epoch}\\n\\n')\n",
    "            logfile.flush()\n",
    "            \n",
    "        logfile.write('=' * 20 + 'end of model' + '=' * 20 + '\\n')\n",
    "        logfile.flush()\n",
    "        \n",
    "        '''Step 3: Test the model'''\n",
    "        test_loss, test_confusion, test_acc, test_f1_pos, test_f1_neg = eval_epoch_binary(\n",
    "            model, test_data_loader, \n",
    "            loss_fn, optimizer, \n",
    "            device, \n",
    "            test_mode = True\n",
    "        )\n",
    "        \n",
    "        logfile.write(f'Test loss = {np.mean(test_loss)}, ')\n",
    "        logfile.write(f'Test accuracy = {test_acc}, ')\n",
    "        logfile.write(f'Test f1_pos = {test_f1_pos}, ')\n",
    "        logfile.write(f'Test f1_neg = {test_f1_neg}\\n')\n",
    "        logfile.write(f'# True negative in test set = {test_confusion[0, 0]}\\n')\n",
    "        logfile.write(f'# False positive in test set = {test_confusion[0, 1]}\\n')\n",
    "        logfile.write(f'# False negative in test set = {test_confusion[1, 0]}\\n')\n",
    "        logfile.write(f'# True positive in test set = {test_confusion[1, 1]}\\n\\n\\n')\n",
    "        \n",
    "        # Append information in the metric table \n",
    "        metric_table.append([model_name, tokenizer, toxic_threshold, test_confusion, test_acc, test_f1_pos, test_f1_neg])\n",
    "\n",
    "logfile.close()\n",
    "print('\\r', end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show final result for comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>encoder</th>\n",
       "      <th>toxic_threshold</th>\n",
       "      <th>test_confusion_matrix</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1_pos</th>\n",
       "      <th>test_f1_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mlp</td>\n",
       "      <td>bow</td>\n",
       "      <td>0.15</td>\n",
       "      <td>[[293, 1108], [728, 10872]]</td>\n",
       "      <td>0.858780</td>\n",
       "      <td>0.922137</td>\n",
       "      <td>0.241949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mlp</td>\n",
       "      <td>bow</td>\n",
       "      <td>0.30</td>\n",
       "      <td>[[385, 1016], [1674, 9926]]</td>\n",
       "      <td>0.793093</td>\n",
       "      <td>0.880667</td>\n",
       "      <td>0.222543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mlp</td>\n",
       "      <td>bow</td>\n",
       "      <td>0.45</td>\n",
       "      <td>[[510, 891], [2072, 9528]]</td>\n",
       "      <td>0.772094</td>\n",
       "      <td>0.865434</td>\n",
       "      <td>0.256088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mlp</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.15</td>\n",
       "      <td>[[1401, 0], [11600, 0]]</td>\n",
       "      <td>0.107761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mlp</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.30</td>\n",
       "      <td>[[0, 1401], [0, 11600]]</td>\n",
       "      <td>0.892239</td>\n",
       "      <td>0.943051</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mlp</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.45</td>\n",
       "      <td>[[1401, 0], [11600, 0]]</td>\n",
       "      <td>0.107761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mlp</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.15</td>\n",
       "      <td>[[56, 1345], [481, 11119]]</td>\n",
       "      <td>0.859549</td>\n",
       "      <td>0.924119</td>\n",
       "      <td>0.057792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mlp</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.30</td>\n",
       "      <td>[[1284, 117], [9967, 1633]]</td>\n",
       "      <td>0.224367</td>\n",
       "      <td>0.244644</td>\n",
       "      <td>0.202972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mlp</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.45</td>\n",
       "      <td>[[1316, 85], [10139, 1461]]</td>\n",
       "      <td>0.213599</td>\n",
       "      <td>0.222273</td>\n",
       "      <td>0.204729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bert</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.15</td>\n",
       "      <td>[[269, 1132], [566, 11034]]</td>\n",
       "      <td>0.869395</td>\n",
       "      <td>0.928553</td>\n",
       "      <td>0.240608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bert</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.30</td>\n",
       "      <td>[[271, 1130], [768, 10832]]</td>\n",
       "      <td>0.854011</td>\n",
       "      <td>0.919447</td>\n",
       "      <td>0.222131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bert</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.45</td>\n",
       "      <td>[[298, 1103], [819, 10781]]</td>\n",
       "      <td>0.852165</td>\n",
       "      <td>0.918157</td>\n",
       "      <td>0.236696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>bow</td>\n",
       "      <td>0.15</td>\n",
       "      <td>[[111, 1290], [584, 11016]]</td>\n",
       "      <td>0.855857</td>\n",
       "      <td>0.921610</td>\n",
       "      <td>0.105916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>bow</td>\n",
       "      <td>0.30</td>\n",
       "      <td>[[1144, 257], [8124, 3476]]</td>\n",
       "      <td>0.355357</td>\n",
       "      <td>0.453401</td>\n",
       "      <td>0.214453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>bow</td>\n",
       "      <td>0.45</td>\n",
       "      <td>[[1305, 96], [9404, 2196]]</td>\n",
       "      <td>0.269287</td>\n",
       "      <td>0.316153</td>\n",
       "      <td>0.215524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.15</td>\n",
       "      <td>[[191, 1210], [992, 10608]]</td>\n",
       "      <td>0.830628</td>\n",
       "      <td>0.905970</td>\n",
       "      <td>0.147833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.30</td>\n",
       "      <td>[[1401, 0], [10872, 728]]</td>\n",
       "      <td>0.163757</td>\n",
       "      <td>0.118105</td>\n",
       "      <td>0.204914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.45</td>\n",
       "      <td>[[1401, 0], [11198, 402]]</td>\n",
       "      <td>0.138682</td>\n",
       "      <td>0.066989</td>\n",
       "      <td>0.200143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.15</td>\n",
       "      <td>[[1124, 277], [8849, 2751]]</td>\n",
       "      <td>0.298054</td>\n",
       "      <td>0.376128</td>\n",
       "      <td>0.197644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.30</td>\n",
       "      <td>[[447, 954], [3473, 8127]]</td>\n",
       "      <td>0.659488</td>\n",
       "      <td>0.785939</td>\n",
       "      <td>0.168014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.45</td>\n",
       "      <td>[[1035, 366], [8447, 3153]]</td>\n",
       "      <td>0.322129</td>\n",
       "      <td>0.417091</td>\n",
       "      <td>0.190205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model_name encoder  toxic_threshold        test_confusion_matrix  \\\n",
       "0                   mlp     bow             0.15  [[293, 1108], [728, 10872]]   \n",
       "1                   mlp     bow             0.30  [[385, 1016], [1674, 9926]]   \n",
       "2                   mlp     bow             0.45   [[510, 891], [2072, 9528]]   \n",
       "3                   mlp   tfidf             0.15      [[1401, 0], [11600, 0]]   \n",
       "4                   mlp   tfidf             0.30      [[0, 1401], [0, 11600]]   \n",
       "5                   mlp   tfidf             0.45      [[1401, 0], [11600, 0]]   \n",
       "6                   mlp    bert             0.15   [[56, 1345], [481, 11119]]   \n",
       "7                   mlp    bert             0.30  [[1284, 117], [9967, 1633]]   \n",
       "8                   mlp    bert             0.45  [[1316, 85], [10139, 1461]]   \n",
       "9                  bert    bert             0.15  [[269, 1132], [566, 11034]]   \n",
       "10                 bert    bert             0.30  [[271, 1130], [768, 10832]]   \n",
       "11                 bert    bert             0.45  [[298, 1103], [819, 10781]]   \n",
       "12  logistic_regression     bow             0.15  [[111, 1290], [584, 11016]]   \n",
       "13  logistic_regression     bow             0.30  [[1144, 257], [8124, 3476]]   \n",
       "14  logistic_regression     bow             0.45   [[1305, 96], [9404, 2196]]   \n",
       "15  logistic_regression   tfidf             0.15  [[191, 1210], [992, 10608]]   \n",
       "16  logistic_regression   tfidf             0.30    [[1401, 0], [10872, 728]]   \n",
       "17  logistic_regression   tfidf             0.45    [[1401, 0], [11198, 402]]   \n",
       "18  logistic_regression    bert             0.15  [[1124, 277], [8849, 2751]]   \n",
       "19  logistic_regression    bert             0.30   [[447, 954], [3473, 8127]]   \n",
       "20  logistic_regression    bert             0.45  [[1035, 366], [8447, 3153]]   \n",
       "\n",
       "    test_accuracy  test_f1_pos  test_f1_neg  \n",
       "0        0.858780     0.922137     0.241949  \n",
       "1        0.793093     0.880667     0.222543  \n",
       "2        0.772094     0.865434     0.256088  \n",
       "3        0.107761     0.000000     0.194556  \n",
       "4        0.892239     0.943051     0.000000  \n",
       "5        0.107761     0.000000     0.194556  \n",
       "6        0.859549     0.924119     0.057792  \n",
       "7        0.224367     0.244644     0.202972  \n",
       "8        0.213599     0.222273     0.204729  \n",
       "9        0.869395     0.928553     0.240608  \n",
       "10       0.854011     0.919447     0.222131  \n",
       "11       0.852165     0.918157     0.236696  \n",
       "12       0.855857     0.921610     0.105916  \n",
       "13       0.355357     0.453401     0.214453  \n",
       "14       0.269287     0.316153     0.215524  \n",
       "15       0.830628     0.905970     0.147833  \n",
       "16       0.163757     0.118105     0.204914  \n",
       "17       0.138682     0.066989     0.200143  \n",
       "18       0.298054     0.376128     0.197644  \n",
       "19       0.659488     0.785939     0.168014  \n",
       "20       0.322129     0.417091     0.190205  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metric_table, columns = colNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeout-message\n",
    "\n",
    "xprmt_05a_another should be compared with xprmt_02 and xprmt_04 because all 3 experiments use the same mixed dataset. \n",
    "\n",
    "<ul>\n",
    "    <li>xprmt_02 uses TF-IDF tokenizer and simple models. xprmt_02 indicates that simple logistic regression will work reasonably well while being computationally cheap. Rate of false negative = 2377 / 11600 = 20%</li>\n",
    "    <li>xprmt_04 uses BERT tokenizer and BERT classifier. BERT classifier has more tendency to predict false postive, but it significantly reduces the rate of false negative to 762 / 11600 = 6.57%</li>\n",
    "    <li>Part of xprmt_05 applies simple models (e.g. logistic regression) with BERT tokenizer. This results in significantly high rate of false negative. <span style=\"color=red;\">BUT I suspect this high rate of false negative is caused by wrong choice of optimizer, because I accidentally forgot to use AdamW for BERT classifier, and the f1-score seems outrageous. The high number of false negative may also be caused by BERT tokenizer.</span></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
